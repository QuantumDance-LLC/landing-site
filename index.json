[{"categories":["python"],"contents":" Saleor is a fast-growing open source e-commerce platform based on Python and Django, and is constantly being updated, so don\u0026rsquo;t worry about the old version.\nIt has the following features:\n GraphQL API: GraphQL-based implementation of the front and back end separation , belonging to the cutting-edge technology . Dashboard: Administrators have full control over users, processes and products. Order: Integrated system for orders, shipments and refunds. Shopping cart: advanced payment and tax options, support for discounts and promotions Payments: Flexible API architecture allows integration of any payment method Geo-Adaptive: Automatic support for multi-country checkout experience Cloud deployment support: Docker deployment support. Support Google Analytics: Integrated with Google Analytics, you can easily analyze traffic going and staying.   Saleor repository address: https://github.com/mirumee/saleor\n 1. Deployment Guide  Saleor supports a variety of ways to run, you can use the manual installation and run the way, you can also use Docker to run, the following is to introduce the platform-wide common and the simplest Docker deployment scheme.\nBefore following the instructions below, you need to install Docker Desktop and Docker Compose, if you haven\u0026rsquo;t done so, you can check out this tutorial:\n https://www.runoob.com/docker/docker-tutorial.html\n Docker deployment of Saleor is very easy, you just need to clone the repository and build the image and run the service.\n# Python Practical Dictionary # Cloning a repository git clone https://github.com/mirumee/saleor-platform.git --recursive --jobs 3 cd saleor-platform # Build the Docker image docker-compose build Saleor uses shared folders to enable live code reloading. If you are using Windows or MacOS, you will need to.\n  Place the cloned saleor-platform directory into Docker\u0026rsquo;s shared directory configuration (Settings -\u0026gt; Shared Drives or Preferences -\u0026gt; Resources -\u0026gt; File sharing).\n  Make sure you have at least 5 GB of dedicated memory in Docker preferences (Settings -\u0026gt; Advanced or Preferences -\u0026gt; Resources -\u0026gt; Advanced)\n  Execute database migrations and package front-end resources.\ndocker-compose run --rm api python3 manage.py migrate docker-compose run --rm api python3 manage.py collectstatic --noinput (Optional) Populate the database with sample data.\ndocker-compose run --rm api python3 manage.py populatedb Finally, create an administrator account for yourself: docker-compose run --rm\ndocker-compose run --rm api python3 manage.py createsuperuser Run the service:\nRun Saleor with the following command:\ndocker-compose up  2. Introduction to the architecture  If you want to develop based on Saleor, then you must understand its architecture.\nSaleor consists of three important components.\n  Saleor Core, which is the backend server for the GraphQL API. Based on Django, it uses PostgreSQL as the database and stores some cached information in Redis.\n  Saleor Dashboard, a dashboard that can be used to run a store. It\u0026rsquo;s a static website, so it doesn\u0026rsquo;t have any backend code of its own, it\u0026rsquo;s a React application that talks to the Saleor Core server.\n  Saleor Storefront, this is the sample store based on React implementation, you can customize this part of the code to meet your own needs or you can build a custom storefront using Saleor SDK.\n  All three components use GraphQL to communicate via HTTPS.\n3. Extended Development  Although you can develop directly from the Saleor source code, it is not officially recommended to do so because if your code conflicts with the official Saleor source code, it will be difficult to keep up with the official updates and you will end up in an awkward situation where no one will maintain the code.\nTherefore Saleor provides two ways to add features:\n  Plug-in functionality: Plug-ins provide an ability to run additional code on Saleor Core and have access to the database.\n  APPS: Develop APP based on GraphQL API and Saleor Core, and also use WebHooks to subscribe to events.\n  Below we describe how to develop extensions based on plugins.\nAs shown above, Saleor Core provides a callback notification event to the plug-in, based on which the plug-in can perform related operations and interact with the database.\nTo develop a plug-in, you must inherit the BasePlugin base class and override some of the methods, such as the following example, which overrides the postprocess_order_creation method to add some operations to the order creation process.\n# Python Utility Dictionary # custom/plugin.py from django.conf import settings from urllib.parse import urljoin from . .base_plugin import BasePlugin from .tasks import api_post_request_task class CustomPlugin(BasePlugin): def postprocess_order_creation(self, order: \u0026#34;Order\u0026#34;, previous_value: Any): # Order creation operations data = ... transaction_url = urljoin(settings.CUSTOM_API_URL, \u0026#34;transactions/createoradjust\u0026#34;) api_post_request_task.delay(transaction_url, data) To load a plugin, you need to configure setup.py to automatically discover the installed plugins. To make the plugins discoverable, you need to set the saleor_plugins field of entry_points, and define the plugins using this syntax: package_name = package_name.path.to:PluginClass.\nThe example is as follows.\n# setup.py from setuptools import setup setup( ... , entry_points={ \u0026#34;saleor.plugins\u0026#34;: [ \u0026#34;my_plugin = my_plugin.plugin:MyPlugin\u0026#34; ] } ) If your plugin is a Django application, the package name (the part before the equal sign) will be added to Django\u0026rsquo;s INSTALLED_APPS so that you can take advantage of Django features such as ORM integration and database migration.\nNotice that our previous order creation operation uses the .delay syntax, which is an asynchronous task for Celery. Since some plugin operations should be done asynchronously, Saleor uses Celery and will find all the asynchronous tasks declared by tasks.py in the plugin directory.\n# custom_plugin/tasks.py import json from celery import shared_task from typing import Any, Dict import requests from requests.auth import HTTPBasicAuth from django.conf import settings @shared_task def api_post_request( url: str, data: Dict[str, Any], ): try: username = \u0026#34;username\u0026#34; password = \u0026#34;password\u0026#34; auth = HTTPBasicAuth(username, password) requests.post(url, auth=auth, data=json.dumps(data), timeout=settings.TIMEOUT) except requests.exceptions.RequestException: return The above api_post_request function is the asynchronous task used by the previous plugin. After the plugin calls the delay method, the task will be stuffed into the queue and executed asynchronously.\nThe above is a simple example of plug-in development, I personally think Saleor\u0026rsquo;s development model is still very good. If you need, you can use this project to build a mall of their own.\nSource: https://mp.weixin.qq.com/s/hHZ_e6qqUWPibuowYFCpHw\n","date":"04 Jan, 2022","image":"images/blog/saleor.webp","permalink":"https://codelink.ai/blog/python/deploy-a-gorgeous-python-open-source-e-commerce-project-saleor/","tags":["deployment","e-commerce"],"title":"Deploy a gorgeous Python open source e-commerce project - Saleor"},{"categories":["data science"],"contents":" DeepFaceLive is a real time face replacement software, one click installation, newbie friendly, and you can\u0026rsquo;t see any mistakes after face replacement.\nDemo  Welcome to \u0026ldquo;DeepFace Live\u0026rdquo;!\nNo need for plastic surgery! No minimally invasive!\nDouble eyelid, eye opening, face slimming are not a problem!\nYou will become a handsome man and a beautiful woman in seconds!\nLet me show you the very mature \u0026ldquo;plastic surgery technology\u0026rdquo;.\nA software can directly transform Angela Baby into Dilraba!\nJust want Dilraba\u0026rsquo;s teardrop and mouth? No problem!\nYou can also change the face of a blessed comedian Shen Teng into that of Jack Ma.\nAnd this time, the \u0026ldquo;plastic surgery technology\u0026rdquo; has a new upgrade!\nReal-time facelift\u0026quot; for live web stars!\nThe face of an internet celebrity is replaced with the face of Fan Bingbing, without any sense of \u0026ldquo;faking\u0026rdquo;.\nThat\u0026rsquo;s right, this is DeepFaceLive\u0026rsquo;s real-time face changing software.\n DeepFaceLive project address: https://github.com/iperov/DeepFaceLive\n Just open the software, you will be able to process the video in real time and give the live broadcaster a new face.\nHere is a big weapon: Liu Yifei model!\nZoom in to see, the five features are perfectly replaced.\nDeepFaceLive is also able to handle different gender face replacement.\nThe software launched by the team this time can change faces in real time for live video, but also during video calls.\nBehind the Scene  Of course, the model of face replacement has to be trained by DeepFaceLab algorithm first.\n Address: https://arxiv.org/pdf/2005.05535.pdf\n More than 95% of the Deep Fake videos on the web are now created with DeepFaceLab.\nFor example, the following popular YouTube channels.\n DeepFaceLab project address: https://github.com/iperov/DeepFaceLab\n With the launch of DeepFaceLive, there will definitely be more fun videos and even live streams.\nThe software is also very easy to run, requiring only a 64-bit Win 10 system and an NV graphics card.\nThat\u0026rsquo;s it!\nDeepFaceLive  Just click and change!\n(Remember to update your graphics card drivers)\nFace Detection The face detector is integrated with YoloV5, S3FD and CenterFace.\nYou can also choose to use the CPU for processing.\nFace Alignment Simple parameters can be modified to adjust the effect of face alignment.\nFace Marker The face tagger provides CPU-based OpenCV LBF and GPU-based Google FaceMesh.\nFace Swapper In the face swapper, you need to load a model trained by the user in advance with DeepFaceLab.\n A detailed tutorial can be found at: https://github.com/iperov/DeepFaceLive/blob/master/doc/setup_tutorial_windows/index.md\n DeepFaceLab  Lab enables smooth and realistic face swapping without the need to hand-pick features.\nOnly two videos are needed: the source video (src) and the target video (dst).\nMoreover, the two videos do not need to match the same facial expression between them.\nPart 1 Extracting faces The first stage of Lab is to extract faces from src and dst data.\nFace Detection In Lab, S3FD is used as the default face detector.\nFace Alignment Lab provides two typical face coordinate extraction algorithms to solve this problem.\nHeat map-based facial coordinate algorithm 2DFAN (for faces with standard pose) PRNet with 3D facial a priori information (for faces with large Euler angles, e.g. one of the sides is out of the line of sight).\nAfter retrieving the face coordinates, Lab provides an optional function with configurable time steps to smooth the face coordinates of consecutive frames in a single shot, further ensuring stability.\nThe classical point pattern mapping and transformation method is then used to compute the similarity transformation matrix for face alignment.\nSince a standard facial coordinate template is required for calculating the similarity transformation matrix, Lab provides a canonical aligned facial coordinate template.\nIn addition, Lab can automatically predict the Euler angles using the obtained facial coordinates.\nFace segmentation After alignment, a folder of face data with standard front or side views is obtained.\nWe use a fine-grained face segmentation network (TernausNet) on this basis to accurately segment faces with hair, finger or glasses occlusions, while also removing irregular occlusions.\nSince some SOTA face segmentation models are unable to generate fine-grained masks in some specific shots, Lab introduced XSeg into the mix.\nXSeg allows users to use multiple photos to train the model to segment specific faces.\nWith the help of XSeg, users can use it to remove the occlusion of hands, glasses and any other objects that may cover the face, and control specific areas for swapping.\nPart 2 Model training Since the authors want to avoid a strict matching of src and dst facial expressions, Lab proposes two structures to solve this problem.\nDF structure LIAE structure The DF structure consists of an encoder and an Inter with shared weights between src and dst, two decoders belonging to src and dst respectively.\nThe generalization of src and dst is achieved by the shared Encoder and Inter.\nThe DF structure can perform the task of face swapping but does not inherit enough information from dst, while the LIAE structure can be used to solve the consistency problem of light.\nThe LIAE structure is more complex, with an encoder that shares weights, a decoder and two separate inputs.\nIn addition, Lab uses hybrid loss (DSSIM+MSE) by default. dSSIM can generate faces faster, while MSE can provide better sharpness.\nIn addition, the authors use a real face mode, TrueFace, which can make the generated faces have better similarity with dst in the conversion stage.\nFrom the results, there is a significant improvement in the quality of the final generated faces.\nPart 3 Face Exchange Previous methods often ignore the importance of the transformation phase.\nLab allows users to swap src faces to dst and reverse the process.\nIn order to maintain consistent face color, Lab provides five more color transfer algorithms (Reinhard color transfer, iterative distribution transfer, etc.).\nLab\u0026rsquo;s liae architecture model comes with light and shadow learning, so when dealing with different skin tones, face shapes and lighting conditions, the combination of the two faces will not look abrupt as long as the edges are feathered.\nFinally, the face is sharpened.\nSince SOTA models produce faces that are more or less smooth and lacking in minute details (e.g., moles, wrinkles), Lab integrates a pre-trained sharpening tool to sharpen faces.\nTherefore, Lab integrates a pre-trained face super-resolution neural network for sharpening the blended faces.\nComparison of results The authors used an open source project from the FaceForensics++ dataset to test the face swapping results.\nExamples of face swapping with different expressions and face shapes\nTo be fair, the authors limited the training time to 3 hours and used a lightweight model with a DF structure: Quick96, which has an output resolution of 96×96.\nIn addition, the authors optimized the model using the Adam optimizer (lr=0.00005, β1=0.5, β2=0.999).\nThe models are trained on NVIDIA GeForce GTX 1080Ti GPUs and Intel Core i7-8700 CPUs.\nFaceForensics++ Qualitative face replacement results for face images\nLab can preserve more poses and expressions than DeepFakes and Nirkin\u0026rsquo;s models.\nIn addition, with the addition of the super-resolution network in the transformation stage, Lab can output more soulful eyes and well-defined teeth.\nHowever, this effect cannot be reflected in the SSIM score.\nReal-time face replacement, is it good or bad?  As researchers continue to pursue natural effects, AI face-swapping technology is becoming more and more \u0026ldquo;impeccable\u0026rdquo;.\nNow the live-streaming industry is riding on the wings of technological development, allowing businesses to profit more.\nThe developer of the software, \u0026ldquo;Rolling Stone, who wishes to remain anonymous,\u0026rdquo; says that if the live-streaming industry can use this face-swapping software, the hosts with interesting souls but not enough faces can have high faces and improve the attractiveness of their live-streaming rooms.\nIt greatly reduces the cost of opening a live room for businesses, while also improving the attractiveness of the live room.\nThe developer also expressed his concern: once the software is widely used, people with bad intentions may use real-time face-swapping technology to commit fraud and blackmail.\nPrevious face-swapping technology can only change the face of the video at most, if you want to fraud, as long as you remain vigilant, the video will soon be revealed.\nBut if real-time face-swapping technology is used for fraud, in this virtual world of real and fake, there may be no way for most people to tell if the end of the screen is the \u0026ldquo;real one\u0026rdquo;.\nIf the scam is for older people like our parents, who can interact with them, they may easily be tricked into transferring money when their discernment is not high.\nIn this AI world, can we still keep the last sincerity between people?\nReference:\nDeepFaceLive project address: https://github.com/iperov/DeepFaceLive\nDeepFaceLab project address: https://github.com/iperov/DeepFaceLab\nDeepFaceLab paper address: https://arxiv.org/pdf/2005.05535.pdf\nSource: https://mp.weixin.qq.com/s/laIhXCx45mhtnn5IYG4mkg\n","date":"04 Jan, 2022","image":"images/blog/deepfacelive.png","permalink":"https://codelink.ai/blog/data-science/github-open-source-magic-let-you-become-a-celebrity-in-one-click/","tags":["computer vision","python"],"title":"GitHub open source magical project! Let you become a celebrity in one click!"},{"categories":["python"],"contents":" Python is not known to be a very efficient language to execute. In addition, looping is a very time-consuming operation in any language. If any simple single-step operation takes 1 unit of time, repeating the operation tens of thousands of times will eventually increase the time spent tens of thousands of times.\nwhile and for are two keywords commonly used in Python to implement loops, there is a real difference in their efficiency. For example, the following test code.\nimport timeit def while_loop(n=100_000_000): i = 0 s = 0 while i \u0026lt; n: s += i i += 1 return s def for_loop(n=100_000_000): s = 0 for i in range(n): s += i return s def main(): print(\u0026#39;while loop\\t\\t\u0026#39;, timeit.timeit(while_loop, number=1)) print(\u0026#39;for loop\\t\\t\u0026#39;, timeit.timeit(for_loop, number=1)) if __name__ == \u0026#39;__main__\u0026#39;: main() # =\u0026gt; while loop 4.718853999860585 # =\u0026gt; for loop 3.211570399813354 This is a simple summation operation that computes the sum of all natural numbers from 1 to n. You can see that the for loop is 1.5 seconds faster than the while loop.\nThe difference is mainly due to the different mechanics of the two.\nIn each loop, while actually performs two more steps than for: a bounds check and the self-incrementing of variable i. That is, for each loop, while performs two more steps than for. That is, for each loop, while does a bounds check (while i \u0026lt; n) and a self-increment calculation (i += 1). Both of these operations are explicitly pure Python code.\nThe for loop does not require a bounds check and self-increment operation, and adds no explicit Python code (pure Python code is less efficient than the underlying C code). When the number of loops is large enough, a significant efficiency gap emerges.\nTwo more functions can be added to add unnecessary bounds checking and self-incrementing to the for loop.\nimport timeit def while_loop(n=100_000_000): i = 0 s = 0 while i \u0026lt; n: s += i i += 1 return s def for_loop(n=100_000_000): s = 0 for i in range(n): s += i return s def for_loop_with_inc(n=100_000_000): s = 0 for i in range(n): s += i i += 1 return s def for_loop_with_test(n=100_000_000): s = 0 for i in range(n): if i \u0026lt; n: pass s += i return s def main(): print(\u0026#39;while loop\\t\\t\u0026#39;, timeit.timeit(while_loop, number=1)) print(\u0026#39;for loop\\t\\t\u0026#39;, timeit.timeit(for_loop, number=1)) print(\u0026#39;for loop with increment\\t\\t\u0026#39;, timeit.timeit(for_loop_with_inc, number=1)) print(\u0026#39;for loop with test\\t\\t\u0026#39;, timeit.timeit(for_loop_with_test, number=1)) if __name__ == \u0026#39;__main__\u0026#39;: main() # =\u0026gt; while loop 4.718853999860585 # =\u0026gt; for loop 3.211570399813354 # =\u0026gt; for loop with increment 4.602369500091299 # =\u0026gt; for loop with test 4.18337869993411 As you can see, the addition of the bounds check and self-increment operations does significantly affect the efficiency of the for loop.\nAs mentioned earlier, Python\u0026rsquo;s underlying interpreter and built-in functions are implemented in C. C is much more efficient than Python.\nFor the above sum of equal variables operation, Python\u0026rsquo;s built-in sum function can be used to obtain a much more efficient execution than a for or while loop.\nimport timeit def while_loop(n=100_000_000): i = 0 s = 0 while i \u0026lt; n: s += i i += 1 return s def for_loop(n=100_000_000): s = 0 for i in range(n): s += i return s def sum_range(n=100_000_000): return sum(range(n)) def main(): print(\u0026#39;while loop\\t\\t\u0026#39;, timeit.timeit(while_loop, number=1)) print(\u0026#39;for loop\\t\\t\u0026#39;, timeit.timeit(for_loop, number=1)) print(\u0026#39;sum range\\t\\t\u0026#39;, timeit.timeit(sum_range, number=1)) if __name__ == \u0026#39;__main__\u0026#39;: main() # =\u0026gt; while loop 4.718853999860585 # =\u0026gt; for loop 3.211570399813354 # =\u0026gt; sum range 0.8658821999561042 As you can see, using the built-in sum function instead of a loop increases the efficiency of code execution exponentially.\nThe sum operation of the built-in function is actually a loop, but it is implemented in C, whereas the sum operation in the for loop is implemented in pure Python code s += i. C \u0026gt; Python.\nExpand your thinking a bit. As a child, you\u0026rsquo;ve heard the story of the childhood Gaussian who cleverly calculated the sum of 1 to 100. 1\u0026hellip;100 equals (1 + 100) * 50. This same calculation can be applied to the summation operation above.\nimport timeit def while_loop(n=100_000_000): i = 0 s = 0 while i \u0026lt; n: s += i i += 1 return s def for_loop(n=100_000_000): s = 0 for i in range(n): s += i return s def sum_range(n=100_000_000): return sum(range(n)) def math_sum(n=100_000_000): return (n * (n - 1)) // 2 def main(): print(\u0026#39;while loop\\t\\t\u0026#39;, timeit.timeit(while_loop, number=1)) print(\u0026#39;for loop\\t\\t\u0026#39;, timeit.timeit(for_loop, number=1)) print(\u0026#39;sum range\\t\\t\u0026#39;, timeit.timeit(sum_range, number=1)) print(\u0026#39;math sum\\t\\t\u0026#39;, timeit.timeit(math_sum, number=1)) if __name__ == \u0026#39;__main__\u0026#39;: main() # =\u0026gt; while loop 4.718853999860585 # =\u0026gt; for loop 3.211570399813354 # =\u0026gt; sum range 0.8658821999561042 # =\u0026gt; math sum 2.400018274784088e-06 The final execution time of math sum is about 2.4e-6, which is a million times shorter. The idea here is that since loops are inefficient, a piece of code has to be executed hundreds of millions of times over.\nSo we just don\u0026rsquo;t need to loop, and use mathematical formulas to turn hundreds of millions of loops into a single step operation. The efficiency is naturally enhanced to an unprecedented degree.\nFinal conclusion (a bit riddler).\n The fastest way to implement loops \u0026ndash; is to not use them\n For Python, use built-in functions whenever possible to minimize the pure Python code in the loop.\nOf course, built-in functions aren\u0026rsquo;t the fastest in some cases. When creating lists, for example, it\u0026rsquo;s the literal writing that\u0026rsquo;s faster.\nSource: https://www.starky.ltd/2021/11/23/the-fastest-way-to-loop-in-python https://mp.weixin.qq.com/s/lJcl-4Xwb9XYEZNG9kuimg\n","date":"04 Jan, 2022","image":"images/blog/python.png","permalink":"https://codelink.ai/blog/python/pythons-fastest-way-to-implement-loops/","tags":["learning"],"title":"Python's fastest way to implement loops (for, while, etc. speed comparison)"},{"categories":["devops"],"contents":" We will share the five application scenarios for Nginx: HTTP server, static server, reverse proxy server, load balancer, and separation of static and dynamic contents.\nI. HTTP server Nginx itself is also a static resource server. When there are only static resources, you can use Nginx to be a server, if a website is only a static page, then it can be deployed in this way.\n1. first in the document root directory Docroot (/usr/local/var/www) to create html directory, and then in the html put a test.html; 2. configure the server in nginx.conf user mengday staff; http { server { listen 80; server_name localhost; client_max_body_size 1024M; # default location location / { root /usr/local/var/www/html; index index.html index.htm; index.html; index.htm; } } } 3. Access test http://localhost/ points to /usr/local/var/www/index.html, index.html is the html that comes with nginx installation http://localhost/test.html points to /usr/local/var/www/html/test.html\n Note: If you get 403 Forbidden errors when accessing images, it may be because the first line of nginx.conf user configuration is not correct, the default is #user nobody; it is commented out, change it to user root under linux; change it to user username group under macos; then reload the configuration file or reboot and try again. The user name can be checked by who am i command.\n 4. Command Introduction  server : used to define the service, there can be multiple server blocks in http listen : Specify the IP address and port for the server to listen to requests, if the address is omitted, the server will listen to all addresses, if the port is omitted, the standard port is used server_name : the name of the service, used to configure the domain name location : The configuration corresponding to the mapped path uri, a server can have multiple locations, location followed by a uri, can be a regular expression, / means match any path, when the client accesses the path meets this uri will execute the code inside the location block root : root path, when visit http://localhost/test.html, \u0026ldquo;/test.html\u0026rdquo; will match to \u0026ldquo;/\u0026rdquo; uri, find root as /usr/local/ var/www/html, the user accesses the resource physical address = root + uri = /usr/local/var/www/html + /test.html = /usr/local/var/www/html/test.html index : set the home page, when only access server_name without any path behind is not to go root directly to the index command; if the access path does not specify a specific file, then return the index set resources, if you visit http://localhost/html/ then the default return index.html  5. location uri regular expression  . : match any character other than the newline character ? : repeat 0 times or 1 times + : repeat 1 or more times * : repeat 0 or more times \\d ：Match a number ^ : Match the beginning of the string $ : Match the end of the string {n} : Repeat n times {n,} : Repeat n or more times [c] : matches a single character c [a-z] : matches any of the lowercase letters a-z (a|b|c) : the genus line means match any of the cases, each case is separated by a vertical line, usually enclosed in parentheses, and matches a character or a b character or a c character \\ Backslash: used to escape special characters  The content matched between the parentheses () can be referenced later by $1, and $2 indicates the content in the second () before. It is easy to confuse people inside the regular \\ escaping special characters.\n II. Static Server In the company often encounter a static server, usually provides a function of upload, other applications if you need static resources from the static server.\n Create images and img directories under /usr/local/var/www, and put a test.jpg under each directory respectively.  http { server { listen 80; server_name localhost; set $doc_root /usr/local/var/www; # default location location / { root /usr/local/var/www/html; index index.html index.htm; index.html; } location ^~ /images/ { root $doc_root; } location ~* \\. (gif|jpg|jpeg|png|bmp|ico|swf|css|js)$ { root $doc_root/img; } } } Custom variables use set directive, syntax set variable name value; reference use variable name value; reference use variable name; here customize doc_root variable.\nThere are two general ways to map static server location.\n Use path, such as /images/ Generally, images are placed in some image directory. Use suffixes such as .jpg, .png, etc. to match the pattern  Visit http://localhost/test.jpg to map to $doc_root/img\nVisit http://localhost/images/test.jpg When the same path meets more than one location, it will match the location with higher priority, because the priority of ^~ is higher than ~, so it will go to the location corresponding to /images/.\nThere are several common location path mapping paths.\n = Exact match for common characters. That is, exact match. ^~ prefix matching. If the match is successful, no other locations will be matched. ~ indicates a regular match, case sensitive ~* means perform a regular match, not case-sensitive /xxx/ regular string path matching /xxx/ generic match, any request will be matched to  Location priority When a path matches multiple locations, there is a priority order of which location can be matched, and the priority order is related to the expression type of the location value, not the order in the configuration file. For the same type of expression, the longer string will be matched first.\nThe following are the descriptions in order of priority.\n The equal sign type (=) has the highest priority. Once the match is successful, no other matches are found and the search stops. ^~ type expression, not a regular expression. Once the match is successful, no more matches are found and the search stops. The regular expression type (~ ~*) has the next highest priority. If more than one location can match, the one with the longest regular expression is used. Regular string match type. Match by prefix. / generic match, if no match, match the generic  Priority search problem: different types of location mapping decide whether to continue down the search\n equals type, ^~ type: once matched, the search stops, no other location will be matched Regular expression type (~ ~*), regular string matching type /xxx/ : after matching, it will continue to search for other locations until it finds the highest priority, or stop searching when it finds the first case  Location priority from highest to lowest:\n(location =) \u0026gt; (location full path) \u0026gt; (location ^~ path) \u0026gt; (location ~,~* regular order) \u0026gt; (location partial start path) \u0026gt; (/)\nlocation = / { # Exact match /, hostname cannot be followed by any string / [ configuration A ] } location / { # Match all requests that start with /. # But if a longer expression of the same type is available, the longer expression is chosen. # If there are regular expressions to match, the regular expressions are matched first. [ configuration B ] } location /documents/ { # Match all requests that start with /documents/, and continue down the list after the match. # But if there is a longer expression of the same type, the longer expression is chosen. # If there are regular expressions to match, the regular expressions are given priority. [ configuration C ] } location ^~ /images/ { # Match all expressions starting with /images/, and if the match is successful, stop matching the lookup and stop searching. # So, even if there is a matching regular expression location, it will not be used [ configuration D ] } location ~* \\. (gif|jpg|jpeg)$ { # Match all requests ending with gif jpg jpeg. # But requests starting with /images/ will use Configuration D, which has a higher priority [ configuration E ] } location /images/ { # Characters matching /images/ will continue to search down [ configuration F ] } location = /test.htm { root /usr/local/var/www/htm; index index.htm; index.htm; }  Note: The priority of location is not related to the location of location configuration\n  III. Reverse Proxy Sever Reverse proxy should be the most used feature of Nginx. Reverse proxy means that the proxy server accepts connection requests on the Internet, forwards the requests to a server on the internal network, and returns the results obtained from the server to the client requesting the connection on the Internet, at which point the proxy server behaves externally as a reverse proxy server.\nSimply put, the real server cannot be directly accessed by the external network, so a proxy server is needed, while the proxy server can be accessed by the external network and the real server in the same network environment, of course, it may be the same server, the port is different.\nReverse proxy through the proxy_pass command to achieve .\nStart a Java Web project with port 8081\nserver { listen 80; server_name localhost; location / { proxy_pass http://localhost:8081; proxy_set_header Host $host:$server_port; # Set user ip address proxy_set_header X-Forwarded-For $remote_addr; # When requesting server error to find another server proxy_next_upstream error timeout invalid_header http_500 http_502 http_503; } http_500 http_502; } When we access localhost, it\u0026rsquo;s the same as accessing localhost:8081\n IV. Load Balancer Load balancing is also a common feature of Nginx. Load balancing means spreading the execution across multiple operating units, such as web servers, FTP servers, enterprise critical application servers, and other mission-critical servers, so that they can work together to complete their tasks.\nSimply put, when there are two or more servers, the requests are randomly distributed to the specified servers according to the rules, and the load balancing configuration generally requires a reverse proxy to be configured at the same time to jump to the load balancing through the reverse proxy. Nginx currently supports three load balancing policies, and two common third-party policies.\nLoad balancing is achieved through the upstream directive. Recommended: Java Interview Questions\n1. RR (round robin :polling by default) Each request is assigned to different back-end servers one by one in chronological order, that is, the first request is assigned to the first server, the second request is assigned to the second server, and if there are only two servers, the third request continues to be assigned to the first one, so that the cycle of polling continues, that is, the ratio of requests received by servers is 1:1, and if the back-end server is down, it can be automatically eliminated. Polling is the default configuration and does not require much configuration\nStart the same project on ports 8081 and 8082 respectively\nupstream web_servers { server localhost:8081; server localhost:8082; } server { listen 80; server_name localhost; #access_log logs/host.access.log main; location / { proxy_pass http://web_servers; # Header Host must be specified proxy_set_header Host $host:$server_port; host $host:$server_port; } } The access address can still get the response http://localhost/api/user/login?username=zhangsan\u0026amp;password=111111, this way is polled\n2. weights Specify the polling rate, weight is proportional to the access ratio, that is, the proportion of requests received by the server is the proportion of the respective configured weight, used in the case of uneven performance of the back-end server, such as the server performance is poor to receive fewer requests, the server performance is better to handle more requests.\nupstream test { server localhost:8081 weight=1; server localhost:8082 weight=3; server localhost:8083 weight=4 backup; backup; } The example is that only one of the four requests is assigned to 8081, and the other three are assigned to 8082. backup means hot standby, which only goes to 8083 if both 8081 and 8082 are down\n3. ip_hash The above two ways have a problem, that is, the next request may be distributed to another server, when our program is not stateless (using the session to save data), then there is a big problem, such as the login information is saved to the session, then jump to another server when you need to log in again So many times we need a client to access only one server, then we need to use iphash, iphash each request by accessing the IP hash result distribution, so that each visitor fixed access to a back-end server, can solve the problem of session.\nupstream test { ip_hash; server localhost:8080; server localhost:8081; } 4. fair (third party) Distribute requests according to the response time of the backend server, with the shorter response time being assigned first. This is configured to give the user a faster response\nupstream backend { fair; server localhost:8080; server localhost:8081; } 5. url_hash (third party) Allocate requests by the hash result of the accessed url, so that each url is directed to the same backend server, which is more effective when the backend server is cached. Add the hash statement to the upstream, the server statement can not write other parameters such as weight, hash_method is the hash algorithm used\nupstream backend { hash $request_uri; hash_method crc32; server localhost:8080; server localhost:8081; } Each of the above five load balancing is applicable to different situations, so you can choose which policy mode to use according to the actual situation, but fair and url_hash need to install third-party modules to use.\n V. Separation of Static and Dynamic Separation of dynamic and static is to let the dynamic web pages in the dynamic website according to certain rules to the unchanging resources and often change the resources to distinguish, dynamic and static resources to do a good job of splitting, we can do caching operations based on the characteristics of static resources, which is the core idea of the static website processing.\nupstream web_servers { server localhost:8081; server localhost:8082; } server { listen 80; server_name localhost; set $doc_root /usr/local/var/www; location ~* \\. (gif|jpg|jpeg|png|bmp|ico|swf|css|js)$ { root $doc_root/img; } location / { proxy_pass http://web_servers; # Header Host must be specified proxy_set_header Host $host:$server_port; host $host:$server_port; } error_page 500 502 503 504 /50x.html; location = /50x.html { root $doc_root; } }  VI. Others 1. return directive Return the http status code and optionally the second parameter can be the redirect URL\nlocation /permanently/moved/url { return 301 http://www.example.com/moved/here; } 2. rewrite directive The rewrite URI request rewrite, which modifies the request URI multiple times during request processing by using the rewrite directive, has one optional parameter and two required parameters.\nThe first (required) parameter is a regular expression that the request URI must match.\nThe second parameter is the URI used to replace the matching URI.\nThe optional third parameter is a flag that can stop further processing of the rewrite directive or send a redirect (code 301 or 302)\nlocation /users/ { rewrite ^/users/(. *)$ /show?user=$1 break; } 3. error_page directive Using the error_page directive, you can configure NGINX to return a custom page with an error code, replace other error codes in the response, or redirect the browser to another URI. in the following example, the error_page directive specifies the page (/404.html) that will return the 404 page error code.\nerror_page 404 /404.html; 4. logs Access log: need to turn on compression gzip on; otherwise no log file is generated, open log_format, access_log comments\nlog_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39; \u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34; \u0026#39; access_log /usr/local/etc/nginx/logs/host.access.log main; gzip on. 5; 5. deny command # Deny access to a directory location ~* \\. (txt|doc)${ root $doc_root; deny all; } 6. Built-in variables The built-in variables that can be used in nginx configuration files start with the dollar sign $, and are also called global variables by some people. The values of some of these predefined variables can be changed. Also, pay attention to the Java Voice public page, reply to \u0026ldquo;back-end interview\u0026rdquo;, and you will be sent a treasure trove of interview questions!\n $args: # This variable is equal to the parameter in the request line, same as $query_string $content_length : The Content-length field in the request header. $content_type : The Content-Type field in the request header. $document_root : The value specified in the root directive for the current request. $host : The request host header field, otherwise it is the server name. $http_user_agent : client-side agent information $http_cookie : client-side cookie information $limit_rate : This variable can limit the connection rate. $request_method : The action requested by the client, usually GET or POST. $remote_addr : IP address of the client. $remote_port : The port of the client. $remote_user : The user name that has been authenticated by Auth Basic Module. $request_filename : The path of the current request file, generated by the root or alias directive with the URI request. $scheme : HTTP method (e.g. http, https). $server_protocol : The protocol used for the request, usually HTTP/1.0 or HTTP/1.1. $server_addr : The server address, this value can be determined after a system call is completed. $server_name : The name of the server. $server_port : The port number on which the request reaches the server. $request_uri : The original URI containing the request parameters, without the host name, e.g. /foo/bar.php?arg=baz. $uri : The current URI without request parameters, $uri does not contain the host name, e.g. /foo/bar.html. $document_uri : Same as $uri  Source: https://mp.weixin.qq.com/s/_J7gUOJK6JWYnkIb4AYK2Q\n","date":"03 Jan, 2022","image":"images/blog/nginx.png","permalink":"https://codelink.ai/blog/devops/five-application-scenarios-for-understanding-nginx-thoroughly/","tags":["nginx"],"title":"Five application scenarios for understanding Nginx thoroughly"},{"categories":["data science"],"contents":" We\u0026rsquo;d like to introduce you to an awesome spatial (geographic) data visualization tool: keplergl.\nKeplergl is completely open source by Uber and is the default tool for spatial data visualization within Uber.\nThrough its open interface package keplergl for Python, we can pass in a variety of formats of data by writing Python code in jupyter notebook, and use its built-in rich spatial data visualization functions in its interactive window embedded in notebook. Here are 3 main addresses for learning.\n  the official website address: https://kepler.gl/\n  jupyter notebook manual address: https://github.com/keplergl/kepler.gl/tree/master/docs/keplergl-jupyter#geojson\n  Case study address: https://github.com/keplergl/kepler.gl/tree/master/bindings/kepler.gl-jupyter/notebooks\n  Installation  The installation of keplergl is very simple.\npip install keplergl Amazing graphics  A wave of stunning graphics are coming.\n Getting Started  import pandas as pd import geopandas as gpd from keplergl import KeplerGl # Create an object kep1 = KeplerGl(height=600) # Activate the object and load it into jupyter notebook kep1 As you can see, after running the basic code in Jupyter directly generated the built-in graphics, the graphics themselves are also dynamic; dark black background is also my favorite:\nAdding data  By default, keplergl can add 3 types of data:\n csv GeoJSON DataFrame  csv format There is a csv data in the local directory: china.csv, which records the latitude and longitude of each province in China.\nwith open(\u0026#34;china.csv\u0026#34;, \u0026#34;r\u0026#34;) as f: csv_data = f.read() # add_data add data kep1.add_data(data=csv_data, name=\u0026#34;csv_kep\u0026#34;) kep1  DataFrame format china = pd.read_csv(\u0026#34;china.csv\u0026#34;) kep1.add_data(data=china, name=\u0026#34;dataframe_kep\u0026#34;) kep1  GeoJson format url = \u0026#39;http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_500k.json\u0026#39; country_gdf = gpd.read_file(url) # geopandas read json file kep1.add_data(data=country_gdf, name=\u0026#34;state\u0026#34;) kep1  Custom graphics  Keplergl\u0026rsquo;s customization method: the criticality button. Once inside, you can customize the operation\n Saving and reusing configurations The configuration of the instantiated kep can be saved and reused in the following instance objects.\n Save.  # Save as a file with open(\u0026#39;config1.py\u0026#39;,\u0026#39;w\u0026#39;) as f: f.write(\u0026#39;config={}\u0026#39;.format(kep1.config)) # Run: magic command %run %run config1.py Reuse  kep2 = KeplerGl(height=400, data={\u0026#34;layer1\u0026#34;:df}, config=kep1.config # configuration of kep1 ) kep2 Save graphics   minimalist version, mainly the file name  kep1.save_to_html(file_name=\u0026#34;first_kep.html\u0026#34;) full version: file name, configuration, data, readability  # 4 parameters kep1.save_to_html(file_name=\u0026#34;first_kep.html\u0026#34;, data={\u0026#39;data_1\u0026#39;:china}, config=config, read_only=True ) Web app  The operations shown above are all done in the notebook, we can also do them directly online: https://kepler.gl/demo\nWe will share more articles after we have studied this tool seriously, this library is worth studying\n Source: https://mp.weixin.qq.com/s/t-X8NtuYXY-lKPOrSY3aPQ\n","date":"02 Jan, 2022","image":"images/blog/post-1.webp","permalink":"https://codelink.ai/blog/data-science/spatial-data-visualization-wizard-keplergl/","tags":["visualization","python"],"title":"Spatial data visualization wizard keplergl"},{"categories":["web development"],"contents":" We recommend several popular websites that can help improve your programming efficiency.\nCodepen  Demo video: https://www.bilibili.com/video/BV1im4y1X7zb/\nHere you can find a lot of front-end code written by coding guru. In addition to various web layout codes, there are more kinds of fun, cool and novel front-end animations and effects.\nFor example, when Christmas is coming, many of you asked if I could draw a Christmas tree with code.\nI can\u0026rsquo;t, but if you type \u0026ldquo;Christmas Tree\u0026rdquo; in this website, you can see different styles of Christmas tree pages!\nClick on the page you like and you\u0026rsquo;ll be taken to the code editing page, where you can modify the HTML, CSS, and JavaScript front-end code and view the results in real time in the area at the bottom, which is very convenient!\nAfter editing a web page, you can browse it in full screen, favorite, clone, share, etc. in the menu at the bottom right corner of the web page, and directly embed the web page into our own project or download the complete code package to local.\nThanks to the development of front-end technology, this site provides developers with a one-stop service to search for projects, edit them online, and share and export them, making it easy for us to learn other people\u0026rsquo;s good code in an independent practice.\nHere are a few more similar sites.\nCodeSandbox  CodeSandbox, as the name suggests, helps you run front-end projects in an isolated environment.\nHere you can create your own sandboxes (projects) based on rich templates for common front-end frameworks like React, Vue, Angular, VuePress, Svelte, etc..\nOnce you create your sandbox, you can edit your code online, view your results in real time, or share your sandbox with other people.\nJSFiddle  JSFiddle is a front-end development practice, you can also write code online and view the effect in real time. Compared with Codepen, I feel that this site has a better editing experience.\nWhen you see a great piece of JS code or plugin on the web, you don\u0026rsquo;t have to download it locally. Paste the code directly into JSFiddle and you\u0026rsquo;ll be able to see it in action in the fastest way possible. Many front-end component libraries now also use this platform to give developers a WYSIWYG experience.\nJSRUN  China\u0026rsquo;s online programming site, in addition to front-end, even supports online debugging and running of up to 30 programming languages!\nLike Codepen, you can see many code snippets written by others here and download them directly. You can also save and share your code, and create your own little code collection.\nI have to say, this site is one of the best in China, with great access and functionality!\nGitpod  This platform is a bit more advanced than the above mentioned sites. It is a powerful online IDE (Integrated Programming Environment) that provides a VSCode-style editor that allows you to write code online to complete your development.\nGitpod is based on container technology and will help you compile, build, and run any GitHub project, not just the front end, with one click! And each project runs in isolation from each other, so you can create them as you go and recycle them whenever you\u0026rsquo;re done.\nIf you\u0026rsquo;ve got your eye on a GitHub project and you don\u0026rsquo;t want to build it locally to see how it works, the best way to build and run it online is with Gitpod. There are more and more GitHub projects that have access to Gitpod, and if you see the button below, you can deploy it with one click, which is much more efficient!\nSource: https://mp.weixin.qq.com/s/VJtMNyhAYROYURfBa1nKfw\n","date":"27 Dec, 2021","image":"images/blog/post-2.webp","permalink":"https://codelink.ai/blog/web-development/a-few-of-this-years-super-hot-programming-sites/","tags":["tools","javascript"],"title":"A few of this year's super-hot programming sites!"},{"categories":["data science"],"contents":" Xgboost is an integrated learning algorithm, which belongs to the category of boosting algorithms in the 3 commonly used integration methods (bagging, boosting, stacking). It is an additive model, and the base model is usually chosen as a tree model, but other types of models such as logistic regression can also be chosen.\n1. xgboost and GBDT  Xgboost belongs to the category of gradient boosted tree (GBDT) models. The basic idea of GBDT is to let the new base model (GBDT takes CART categorical regression tree as the base model) to fit the deviation of the previous model, so as to continuously reduce the deviation of the additive model.\nCompared with the classical GBDT, xgboost has made some improvements, resulting in significant improvements in effectiveness and performance (highlighting a common interview test).\n  GBDT expands the objective function Taylor to the first order, while xgboost expands the objective function Taylor to the second order. More information about the objective function is retained, which helps to improve the effect.\n  GBDT is finding a new fit label for the new base model (negative gradient of the previous additive model), while xgboost is finding a new objective function for the new base model (second-order Taylor expansion of the objective function about the new base model).\n  xgboost adds and L2 regularization term for the leaf weights, thus facilitating the model to obtain a lower variance.\n  xgboost adds a strategy to automatically handle missing value features. By dividing the samples with missing values into left subtree or right subtree respectively and comparing the advantages and disadvantages of the objective functions under the two schemes, the samples with missing values can be divided automatically without the need of filling preprocessing the missing features.\n  In addition, xgboost also supports candidate quantile cuts, feature parallelism, etc., which can improve performance.\n2. xgboost Basic Principle  The following is a general introduction to the principle of xgboost from three perspectives: assumption space, objective function, and optimization algorithm.\n1. Hypothesis space  2. Objective function  3. Optimization algorithm The basic idea: greedy method, learning tree by tree, each tree fits the deviation of the previous model.\nThird, the first t trees to learn what? To finish building the xgboost model, we need to determine some of the following things.\n  how to boost? If the additive model composed of the previous t-1 trees has been obtained, how to determine the learning goal of the tth tree?\n  How to generate the tree? If the learning goal of the tth tree is known, how to learn this tree? Specifically, does it involve splitting or not? Which feature is selected for splitting? What splitting point is chosen? How to take the value of the split leaf nodes?\n  We first consider the problem of how to boost, and then solve the problem of how to take the values of the split leaf nodes.\n 4. How to generate the tth tree?  xgboost uses a binary tree, and at the beginning, all the samples are on one leaf node. Then the leaf nodes are continuously bifurcated to gradually generate a tree.\nxgboost uses a levelwise generation strategy, i.e., it tries to split all the leaf nodes at the same level at a time.\nThe process of splitting leaf nodes to generate a tree has several basic questions: should we split? Which feature to choose for splitting? At what point of the feature to split? and what values are taken on the new leaves after the split?\nThe problem of taking values of leaf nodes has been solved earlier. Let\u0026rsquo;s focus on a few remaining questions.\n Should splitting be performed?  Depending on the pruning strategy of the tree, this problem is handled in two different ways. If it is a prepruning strategy, then splitting will be done only if there is some way of splitting that makes the objective function drop after splitting.\nHowever, if it is a post-pruning strategy, the splitting will be done unconditionally, and then after the tree generation is completed, the branches of the tree will be checked from top to bottom to see if they contribute positively to the decline of the objective function and thus pruned.\nxgboost uses a prepruning strategy and splits only if the gain after splitting is greater than 0.\nWhat features are selected for splitting?  xgboost uses feature parallelism to select the features to be split, i.e., it uses multiple threads to try to use each feature as a splitting feature, find the optimal splitting point for each feature, calculate the gain generated after splitting them, and select the feature with the largest gain as the splitting feature.\nWhat splitting point is selected?  There are two methods for xgboost to select the splitting point of a feature, one is the global scan method and the other is the candidate splitting point method.\nThe global scan method arranges all the values of the feature in the sample from smallest to largest, and tries all the possible splitting locations to find the one with the greatest gain, whose computational complexity is proportional to the number of different values of the sample feature on the leaf node.\nIn contrast, the candidate split point method is an approximate algorithm that selects only a constant number (e.g., 256) of candidate split positions, and then finds the best one from the candidate split positions.\n5. Example of xgboost usage  You can use pip to install xgboost\npip install xgboost The following is an example of using xgboost, you can refer to modify the use.\nimport numpy as np import pandas as pd import xgboost as xgb import datetime from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def printlog(info): nowtime = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d%H:%M:%S\u0026#39;) print(\u0026#34;\\n\u0026#34;+\u0026#34;==========\u0026#34;*8 + \u0026#34;%s\u0026#34;%nowtime) print(info+\u0026#39;... \\n\\n\u0026#39;) # ================================================================================ # I. Reading data # ================================================================================ printlog(\u0026#34;step1: reading data...\u0026#34;) # read dftrain,dftest breast = datasets.load_breast_cancer() df = pd.DataFrame(breast.data,columns = [x.replace(\u0026#39; \u0026#39;,\u0026#39;_\u0026#39;) for x in breast.feature_names]) df[\u0026#39;label\u0026#39;] = breast.target dftrain,dftest = train_test_split(df) xgb_train = xgb.DMatrix(dftrain.drop(\u0026#34;label\u0026#34;,axis = 1),dftrain[[\u0026#34;label\u0026#34;]]) xgb_valid = xgb.DMatrix(dftest.drop(\u0026#34;label\u0026#34;,axis = 1),dftest[[\u0026#34;label\u0026#34;]]) # ================================================================================ # Two, set the parameters # ================================================================================ printlog(\u0026#34;step2: setting parameters...\u0026#34;) num_boost_rounds = 100 early_stopping_rounds = 20 # Configure xgboost model parameters params_dict = dict() # booster parameters params_dict[\u0026#39;learning_rate\u0026#39;] = 0.05 # Learning rate, usually smaller is better. params_dict[\u0026#39;objective\u0026#39;] = \u0026#39;binary:logistic\u0026#39; # tree parameters params_dict[\u0026#39;max_depth\u0026#39;] = 3 # depth of the tree, usually between [3,10] params_dict[\u0026#39;min_child_weight\u0026#39;] = 30 # minimum leaf node sample weight sum, the larger the model the more conservative. params_dict[\u0026#39;gamma\u0026#39;]= 0 # Minimum drop value of loss function required for node splitting, the larger the model the more conservative. params_dict[\u0026#39;subsample\u0026#39;]= 0.8 # horizontal sampling, sample sampling ratio, usually between [0.5, 1].  params_dict[\u0026#39;colsample_bytree\u0026#39;] = 1.0 # longitudinal sampling, feature sampling ratio, usually between [0.5, 1  params_dict[\u0026#39;tree_method\u0026#39;] = \u0026#39;hist\u0026#39; # strategy for constructing the tree, can be auto, exact, approx, hist # regulazation parameters  # Omega(f) = gamma*T + reg_alpha* sum(abs(wj)) + reg_lambda* sum(wj**2)  params_dict[\u0026#39;reg_alpha\u0026#39;] = 0.0 #L1 weight coefficient of the regularization term, the larger the model the more conservative it is, usually takes a value between [0,1]. params_dict[\u0026#39;reg_lambda\u0026#39;] = 1.0 #L2 The weight coefficient of the regularization term, the larger the model the more conservative it is, usually takes a value between [1,100]. # Other parameters params_dict[\u0026#39;eval_metric\u0026#39;] = \u0026#39;auc\u0026#39; params_dict[\u0026#39;silent\u0026#39;] = 1 params_dict[\u0026#39;nthread\u0026#39;] = 2 params_dict[\u0026#39;scale_pos_weight\u0026#39;] = 1 # Setting to positive value for unbalanced samples will make the algorithm converge faster. params_dict[\u0026#39;seed\u0026#39;] = 0 # ================================================================================ # Third, train the model # ================================================================================ printlog(\u0026#34;step3: training model...\u0026#34;) result = {} watchlist = [(xgb_train, \u0026#39;train\u0026#39;),(xgb_valid, \u0026#39;valid\u0026#39;)] bst = xgb.train(params = params_dict, dtrain = xgb_train, num_boost_round = num_boost_round, verbose_eval= 1, evals = watchlist, early_stopping_rounds = early_stopping_rounds, evals_result = result) # ================================================================================ # IV, Evaluation Model # ================================================================================ printlog(\u0026#34;step4: evaluating model ...\u0026#34;) y_pred_train = bst.predict(xgb_train, ntree_limit=bst.best_iteration) y_pred_test = bst.predict(xgb_valid, ntree_limit=bst.best_iteration) print(\u0026#39;train accuracy: {:.5}\u0026#39;.format(accuracy_score(dftrain[\u0026#39;label\u0026#39;], y_pred_train\u0026gt;0.5))) print(\u0026#39;valid accuracy: {:.5}\\n\u0026#39;.format(accuracy_score(dftest[\u0026#39;label\u0026#39;], y_pred_test\u0026gt;0.5))) %matplotlib inline %config InlineBackend.figure_format = \u0026#39;svg\u0026#39; dfresult = pd.DataFrame({(dataset+\u0026#39;_\u0026#39;+feval): result[dataset][feval] for dataset in [\u0026#34;train\u0026#34;, \u0026#34;valid\u0026#34;] for feval in [\u0026#39;auc\u0026#39;]}) dfresult.index = range(1,len(dfresult)+1) ax = dfresult.plot(kind=\u0026#39;line\u0026#39;,figsize=(8,6),fontsize = 12,grid = True) ax.set_title(\u0026#34;Metric During Training\u0026#34;,fontsize = 12) ax.set_xlabel(\u0026#34;Iterations\u0026#34;,fontsize = 12) ax.set_ylabel(\u0026#34;auc\u0026#34;,fontsize = 12) ax = xgb.plot_importance(bst,importance_type = \u0026#34;gain\u0026#34;,xlabel=\u0026#39;Feature Gain\u0026#39;) ax.set_xlabel(\u0026#34;Feature Gain\u0026#34;,fontsize = 12) ax.set_ylabel(\u0026#34;Features\u0026#34;,fontsize = 12) fig = ax.get_figure() fig.set_figwidth(8) fig.set_figheight(6) # ================================================================================ # v. Save the model # ================================================================================ printlog(\u0026#34;step5: saving model ...\u0026#34;) model_dir = \u0026#34;data/bst.model\u0026#34; print(\u0026#34;model_dir: %s\u0026#34;%model_dir) bst.save_model(model_dir) bst_loaded = xgb.Booster(model_file=model_dir) printlog(\u0026#34;task end...\u0026#34;) Source: https://mp.weixin.qq.com/s/eAVcbnsh9zzVnlOFz9w5Dw\n","date":"22 Nov, 2021","image":"images/blog/post-6.webp","permalink":"https://codelink.ai/blog/data-science/a-30-minute-guide-to-xgboost/","tags":["algorithm","python"],"title":"A 30-minute's guide to XGBoost (Python code)"},{"categories":["data science"],"contents":" KalidoKit is the integration of a variety of algorithms to achieve, Facemesh, Blazepose, Handpose, Holistic. Let\u0026rsquo;s see the effect.\nThe virtual image is driven by the movements of real human limbs, faces and hands.\nThe mainstream application direction of this technology is virtual anchor.\nIt is possible to drive avatars to dance.\nIt can also capture the whole body movements, facial expressions, gestures, etc., like the motion picture at the beginning.\nIn addition to this type of driving virtual image type, you can also use your imagination to make some interesting small applications.\nKalidoKit  This project is based on Tensorflow.js implementation.\n Project address: https://github.com/yeemachine/kalidokit\n The key point information captured can be used to drive 2D and 3D avatars, combined with some avatar driving engines, to achieve the effect shown at the beginning of the article.\nIt is possible to drive both Live2D images and 3D VRM images.\nThe technical points involved here can\u0026rsquo;t be finished in one article, so today we mainly talk about the basic key point detection technologies: face key point detection, human pose estimation, and gesture pose estimation.\nFace keypoint detection  Face keypoint detection, there are sparse and dense.\nLike the basic one, 68 keypoints are detected.\nGenerally speaking, for the detection of closed eyes, head posture, open and closed mouth, a simple 68 keypoints is enough.\nOf course, there are also more dense keypoints detection.\nFor some skin beauty applications, a dense keypoint detection algorithm is needed, with thousands of keypoints.\nBut the idea of the algorithm is the same, to return the location coordinates of these keypoints, usually used with face detection algorithms.\nFor those who want to learn face keypoint detection algorithms, we recommend two introductory projects.\n  https://github.com/1adrianb/face-alignment https://github.com/ChanChiChoi/awesome-Face_Recognition   One is a basic introductory project, and the other integrates the mainstream algorithms for face keypoints.\nHuman Pose Estimation  Human pose estimation is also a very basic problem in computer vision.\nFrom the point of view of the name, it can be understood as the estimation of the position of the \u0026ldquo;human body\u0026rdquo; pose (key points, such as head, left hand, right foot, etc.).\nGenerally, there are 4 types of tasks.\n Single-Person Skeleton Estimation (SPSE) Multi-person Pose Estimation Video Pose Tracking 3D Skeleton Estimation  Simply put, it is the detection of human skeleton joint points to locate the human pose.\nHuman pose estimation has a wide range of applications, for example, pose detection and action prediction of pedestrians in street scenes in the autonomous driving industry; pedestrian re-identification problems in the security field, specific action monitoring in special scenes; movie special effects in the film industry, etc.\nFor those who want to learn, you can read this compiled paper at\n https://github.com/cbsudux/awesome-human-pose-estimation\n Gestural posture estimation  Hand joints are more flexible, agile and self-obscuring, so it is a little more complicated.\nBut the principle is similar to human posture estimation.\nIn addition to this regular gesture recognition, it can also be used to do some special effects.\nIn fact, many of these human effects, the positioning of the position, are achieved with the help of these key points.\nAs above, to learn, you can see this integrated material at\n https://github.com/xinghaochen/awesome-hand-pose-estimation\n Source: https://mp.weixin.qq.com/s/t3qOKkErm8ZbmKDOhHqCOA\n","date":"22 Nov, 2021","image":"images/blog/post-5.png","permalink":"https://codelink.ai/blog/data-science/algorithm-kalidokit/","tags":["computer vision","python"],"title":"KalidoKit: algorithms to achieve, Facemesh, Blazepose, Handpose, Holistic"},{"categories":["python"],"contents":" This article will focus on the threading module, and for everyday developers, this content is a must-have, and also a high frequency interview FAQ.\n Official documentation (https://docs.python.org/zh-cn/3.6/library/threading.html)\n Thread safety  Thread safety is a concept in multi-threaded or multi-process programming. In a program where multiple threads with shared data are executed in parallel, thread-safe code will ensure that each thread is executed properly and correctly through a synchronization mechanism, without data contamination or other unexpected situations.\nFor example, if there are 10 candies (resources) in a room (process), and there are 3 villains (1 main thread and 2 sub-threads), when villain A eats 3 candies and is forced to rest by the system, he thinks there are 7 candies left, and when villain B eats 3 candies after working, then when villain A comes back on duty, he thinks there are 7 candies left, but in fact there are only 4.\nThe above example where the data of thread A and thread B are not synchronized is a thread safety issue which can lead to very serious surprises, let\u0026rsquo;s go by the following example.\nHere we have a value num with an initial value of 0. We open 2 threads.\n  Thread 1 performs a 10 million + 1 operation on num\n  Thread 2 performs a -1 operation on num 10 million times\n  The result may be staggering, as num does not end up being 0 as we thought.\nimport threading num = 0 def add(): global num for i in range(10_000_000): num += 1 def sub(): global num for i in range(10_000_000): num -= 1 if __name__ == \u0026#34;__main__\u0026#34;: subThread01 = threading.Thread(target=add) subThread02 = threading. subThread01.start() subThread02.start() subThread01.join() subThread02.join() print(\u0026#34;num result : %s\u0026#34; % num) # The results are collected three times # num result : 669214 # num result : -1849179 # num result : -525674 This is a very good case above, and to solve this problem we have to secure the timing of thread switching through locks.\nIt is worth noting that the Python basic data types list, tuple, and dict are thread-safe, so if there are multiple threads operating on these three containers, we don\u0026rsquo;t need to consider thread-safety issues.\nThe role of locks  Locks are a means by which Python provides us with the ability to manipulate thread switching on our own, and they can be used to make thread switching orderly.\nOnce thread switching is ordered, access and modification of data between threads becomes controlled, so to ensure thread safety, locks must be used.\nThe threading module provides the five most common types of locks, which are divided by function as follows.\n synchronous locks: lock (only one can be released at a time) recursive locks: rlock (only one can be released at a time) conditional locks: condition (any one can be released at a time) Event lock: event (all at once) semaphore lock: semaphore (can release a specific one at a time)  1. Lock() synchronous lock  Basic introduction Lock lock has many names, such as.\n Synchronous lock Mutual exclusion lock  What do they mean? As follows.\n  Mutual exclusion means that a resource can be accessed by only one visitor at the same time, and is unique and exclusive, but mutual exclusion cannot restrict the order of access to the resource by the visitor, i.e., the access is unordered\n  Synchronization means that on the basis of mutual exclusion (in most cases), other mechanisms are used to achieve orderly access to resources by visitors\n  Synchronization is actually a more complex implementation of mutual exclusion, because it implements orderly access on top of mutual exclusion\n  The following methods are provided by the threading module in connection with synchronous locks.\n   Method Description     threading.Lock() returns a synchronous lock object   lockObject.acquire(blocking=True, timeout=1) lock, when a thread is executing the locked block, it will not be allowed to switch to other threads, the default lock expiration time is 1 second   lockObject.release() Unlock, when a thread is executing an unlocked block, it will allow the system to switch to other threads according to the policy   lockObject.locked() determines whether the lock object is locked or not, and returns a boolean value    Usage  Synchronous locks can only release one thread at a time. A locked thread will not surrender execution rights while running, but will only hand over execution rights to other threads through system scheduling when the thread is unlocked.\nThe top problem is solved using synchronous locking as follows.\nimport threading num = 0 def add(): lock.acquire() global num for i in range(10_000_000): num += 1 lock.release() def sub(): lock.acquire() global num for i in range(10_000_000): num -= 1 lock.release() if __name__ == \u0026#34;__main__\u0026#34;: lock = threading.Lock() subThread01 = threading.Thread(target=add) subThread02 = threading. subThread01.start() subThread02.start() subThread01.join() subThread02.join() print(\u0026#34;num result : %s\u0026#34; % num) # The results are collected three times # num result : 0 # num result : 0 # num result : 0 This makes the code completely serial, which is not as fast as directly using serialized single-threaded execution for such computationally intensive I/O operations, so this example is only meant as an example and does not outline the real use of locks.\nDeadlock phenomenon  For synchronous locks, one acquire() must correspond to one release(), and the operation of using multiple acquires() followed by multiple releases() cannot be repeated continuously, which will cause a deadlock causing the program to block and not move at all, as follows.\nimport threading num = 0 def add(): lock.acquire() # locking lock.acquire() # deadlock # Do not execute global num for i in range(10_000_000): num += 1 lock.release() lock.release() def sub(): lock.acquire() # locking lock.acquire() # deadlock # Do not execute global num for i in range(10_000_000): num -= 1 lock.release() lock.release() if __name__ == \u0026#34;__main__\u0026#34;: lock = threading.Lock() subThread01 = threading.Thread(target=add) subThread02 = threading. subThread01.start() subThread02.start() subThread01.join() subThread02.join() print(\u0026#34;num result : %s\u0026#34; % num) The with statement  Since the __enter__() and __exit__() methods are implemented in the threading.Lock() object, we can use the with statement to perform context-managed locking and unlocking operations in the following way.\nimport threading num = 0 def add(): with lock: # auto-lock global num for i in range(10_000_000): num += 1 # Auto-unlock def sub(): with lock: # Auto-lock global num for i in range(10_000_000): num -= 1 # Auto-unlock if __name__ == \u0026#34;__main__\u0026#34;: lock = threading.Lock() subThread01 = threading.Thread(target=add) subThread02 = threading. subThread01.start() subThread02.start() subThread01.join() subThread02.join() print(\u0026#34;num result : %s\u0026#34; % num) # The results are collected three times # num result : 0 # num result : 0 # num result : 0 2. RLock() Recursive lock  Basic introduction Recursive locking is an upgraded version of synchronous locking, which can be done on the basis of synchronous locking by repeatedly using acquire() and then repeatedly using release(), but it must be noted that the number of locks and unlocks must be the same, otherwise it will also cause deadlock phenomenon.\nThe following methods are provided by the threading module with recursive locks.\n   Method Description     threading.RLock() returns a recursive lock object   lockObject.acquire(blocking=True, timeout=1) lock, when a thread is executing the locked block, it will not be allowed to switch to other threads, the default lock expiration time is 1 second   lockObject.release() Unlock, when a thread is executing an unlocked block, it will allow the system to switch to other threads according to the policy   lockObject.locked() determines whether the lock object is locked or not, and returns a boolean value    Usage The following is a simple use of recursive locking. If you use synchronous locking, deadlocking will occur, but recursive locking will not.\nimport threading num = 0 def add(): lock.acquire() lock.acquire() global num for i in range(10_000_000): num += 1 lock.release() lock.release() def sub(): lock.acquire() lock.acquire() global num for i in range(10_000_000): num -= 1 lock.release() lock.release() if __name__ == \u0026#34;__main__\u0026#34;: lock = threading.RLock() subThread01 = threading.Thread(target=add) subThread02 = threading. subThread01.start() subThread02.start() subThread01.join() subThread02.join() print(\u0026#34;num result : %s\u0026#34; % num) # The results are collected three times # num result : 0 # num result : 0 # num result : 0 The with statement Since the __enter__() and __exit__() methods are implemented in the threading.RLock() object, we can use the with statement to perform context-managed locking and unlocking operations in the form of\nimport threading num = 0 def add(): with lock: # auto-lock global num for i in range(10_000_000): num += 1 # Auto-unlock def sub(): with lock: # Auto-lock global num for i in range(10_000_000): num -= 1 # Auto-unlock if __name__ == \u0026#34;__main__\u0026#34;: lock = threading.RLock() subThread01 = threading.Thread(target=add) subThread02 = threading. subThread01.start() subThread02.start() subThread01.join() subThread02.join() print(\u0026#34;num result : %s\u0026#34; % num) # The results are collected three times # num result : 0 # num result : 0 # num result : 0 3. Condition() Condition lock  Basic introduction Condition lock is based on the recursive lock to add the function to suspend the running of the thread. And we can use wait() and notify() to control the number of threads executed.\nNote: Conditional locks can be freely set to release several threads at a time.\nThe following methods are provided by the threading module and the conditional lock.\n   Method Description     threading.Condition() returns a conditional lock object   lockObject.acquire(blocking=True, timeout=1) lock, when a thread is executing the locked block, it will not be allowed to switch to another thread, the default lock expiration time is 1 second   lockObject.release() Unlock, when a thread is executing an unlocked block, it will allow the system to switch to other threads according to the policy   lockObject.wait(timeout=None) sets the current thread to a \u0026ldquo;wait\u0026rdquo; state, which will only continue after the thread is \u0026ldquo;notified\u0026rdquo; or the timeout expires. The thread in the \u0026ldquo;wait\u0026rdquo; state will allow the system to switch to other threads according to the policy   lockObject.wait_for(predicate, timeout=None) sets the current thread to the \u0026ldquo;waiting\u0026rdquo; state, and will only continue to run after the thread\u0026rsquo;s predicate returns a True or the timeout expires. The thread in the \u0026ldquo;waiting\u0026rdquo; state will allow the system to switch to other threads according to the policy. Note: the predicate parameter should be passed as a callable object and return a bool type result   lockObject.notify(n=1) notifies a thread with the current status of \u0026ldquo;waiting\u0026rdquo; to continue running, or multiple threads with the n parameter   lockObject.notify_all() notifies all threads whose current state is \u0026ldquo;waiting\u0026rdquo; to continue running    Usage The following example starts 10 sub-threads and immediately sets the 10 sub-threads to the waiting state.\nThen we can send one or more notifications to resume the waiting subthreads.\nimport threading currentRunThreadNumber = 0 maxSubThreadNumber = 10 def task(): global currentRunThreadNumber thName = threading.currentThread().name condLock.acquire() # lock print(\u0026#34;start and wait run thread : %s\u0026#34; % thName) condLock.wait() # suspend the thread and wait to wake it up currentRunThreadNumber += 1 print(\u0026#34;carry on run thread : %s\u0026#34; % thName) condLock.release() # unlock if __name__ == \u0026#34;__main__\u0026#34;: condLock = threading.Condition() for i in range(maxSubThreadNumber): subThreadIns = threading.Thread(target=task) subThreadIns.start() while currentRunThreadNumber \u0026lt; maxSubThreadNumber: notifyNumber = int( input(\u0026#34;Please enter the number of threads that need to be notified to run:\u0026#34;)) condLock.acquire() condLock.notify(notifyNumber) # release condLock.release() print(\u0026#34;main thread run end\u0026#34;) # Start 10 subthreads first, then all of them will become waiting # start and wait run thread : Thread-1 # start and wait run thread : Thread-2 # start and wait run thread : Thread-3 # start and wait run thread : Thread-4 # start and wait run thread : Thread-5 # start and wait run thread : Thread-6 # start and wait run thread : Thread-7 # start and wait run thread : Thread-8 # start and wait run thread : Thread-9 # start and wait run thread : Thread-10 # Batch send notification to release a specific number of sub threads to continue running # Please enter the number of threads that need to be notified to run: 5 # Release 5 # carry on run thread : Thread-4 # carry on run thread : Thread-3 # carry on run thread : Thread-1 # carry on run thread : Thread-2 # carry on run thread : Thread-5 # Please enter the number of threads that need to be notified to run : 5 # release 5 # carry on run thread : Thread-8 # carry on run thread : Thread-10 # carry on run thread : Thread-6 # carry on run thread : Thread-9 # carry on run thread : Thread-7 # Please enter the number of threads that need to be notified to run: 1 # main thread run end with statement Since the __enter__() and __exit__() methods are implemented in the threading.Condition() object, we can use the with statement to perform context-managed locking and unlocking operations in the form of\nimport threading currentRunThreadNumber = 0 maxSubThreadNumber = 10 def task(): global currentRunThreadNumber thName = threading.currentThread().name with condLock: print(\u0026#34;start and wait run thread : %s\u0026#34; % thName) condLock.wait() # suspend the thread and wait to wake it up currentRunThreadNumber += 1 print(\u0026#34;carry on run thread : %s\u0026#34; % thName) if __name__ == \u0026#34;__main__\u0026#34;: condLock = threading. for i in range(maxSubThreadNumber): subThreadIns = threading.Thread(target=task) subThreadIns.start() while currentRunThreadNumber \u0026lt; maxSubThreadNumber: notifyNumber = int( input(\u0026#34;Please enter the number of threads that need to be notified to run:\u0026#34;)) with condLock: condLock.notify(notifyNumber) # Release print(\u0026#34;main thread run end\u0026#34;) 4. Event() event lock  Basic introduction Event lock is based on conditional locking. The difference between it and conditional locking is that it can only release all the threads at once, and cannot release any number of child threads to continue running.\nWe can think of event lock as a traffic light, when the light is red all sub-threads are suspended and enter the \u0026ldquo;waiting\u0026rdquo; state, when the light is green all sub-threads are back to \u0026ldquo;running\u0026rdquo;.\nThe following methods are provided by the threading module in relation to the event lock.\n   Method Description     threading.Event() returns an event lock object   lockObject.clear() sets the event lock to a red light, i.e. all threads are suspended   lockObject.is_set() is used to determine the current event lock status, red is False, green is True   lockObject.set() sets the event lock to a green state, i.e. all threads resume running   lockObject.wait(timeout=None) sets the current thread to the \u0026ldquo;wait\u0026rdquo; state, which will continue to run only after the thread receives the \u0026ldquo;green light\u0026rdquo; or the timeout expires. The thread in the \u0026ldquo;wait\u0026rdquo; state will allow the system to switch to other threads according to the policy.    Usage Event locks cannot be used with the with statement, only in the usual way.\nLet\u0026rsquo;s simulate the operation of a thread and a traffic light, stop on red and go on green as follows\nimport threading maxSubThreadNumber = 3 def task(): thName = threading.currentThread().name print(\u0026#34;start and wait run thread : %s\u0026#34; % thName) eventLock.wait() # pause run and wait for green light print(\u0026#34;green light, %scarry on run\u0026#34; % thName) print(\u0026#34;red light, %sstop run\u0026#34; % thName) eventLock.wait() # pause run, wait for green light print(\u0026#34;green light, %scarry on run\u0026#34; % thName) print(\u0026#34;sub thread %srun end\u0026#34; % thName) if __name__ == \u0026#34;__main__\u0026#34;: eventLock = threading.Event() for i in range(maxSubThreadNumber): subThreadIns = threading.Thread(target=task) subThreadIns.start() eventLock.set() # set to green eventLock.clear() # set to red eventLock.set() # set to green # start and wait run thread : Thread-1 # start and wait run thread : Thread-2 # start and wait run thread : Thread-3 # green light, Thread-1 carry on run # red light, Thread-1 stop run # green light, Thread-1 carry on run # sub thread Thread-1 run end # green light, Thread-3 carry on run # red light, Thread-3 stop run # green light, Thread-3 carry on run # sub thread Thread-3 run end # green light, Thread-2 carry on run # red light, Thread-2 stop run # green light, Thread-2 carry on run # sub thread Thread-2 run end 5. Semaphore() semaphore lock  Basic Introduction A semaphore lock is also based on a conditional lock. It differs from a conditional lock and an event lock as follows.\nConditional lock: You can release any thread that is in the \u0026ldquo;waiting\u0026rdquo; state at one time.\nEvent lock: All threads in the \u0026ldquo;waiting\u0026rdquo; state can be released at once.\nSemaphore locks: a specified number of threads can be released in a batch in a \u0026ldquo;locked\u0026rdquo; state.\nThe following methods are provided by the threading module in relation to semaphore locks.\n   Method Description     threading.Semaphore() returns a semaphore lock object   lockObject.acquire(blocking=True, timeout=1) lock, when a thread is executing a locked block, it will not be allowed to switch to another thread, the default lock expiration time is 1 second   lockObject.release() Unlock, when a thread is executing an unlocked block, it will allow the system to switch to other threads according to the policy    Usage The following is a sample usage, which you can use as a width-limited section where only the same number of threads can be released at a time.\nimport threading import time maxSubThreadNumber = 6 def task(): thName = threading.currentThread().name semaLock.acquire() print(\u0026#34;run sub thread %s\u0026#34; % thName) time.sleep(3) semaLock.release() if __name__ == \u0026#34;__main__\u0026#34;: # Only 2 can be released at a time semaLock = threading.Semaphore(2) for i in range(maxSubThreadNumber): subThreadIns = threading.Thread(target=task) subThreadIns.start() # run sub thread Thread-1 # run sub thread Thread-2 # run sub thread Thread-3 # run sub thread Thread-4 # run sub thread Thread-6 # run sub thread Thread-5 The with statement Since the __enter__() and __exit__() methods are implemented in the threading.Semaphore() object, we can use the with statement to perform context-managed locking and unlocking operations.\nimport threading import time maxSubThreadNumber = 6 def task(): thName = threading.currentThread().name with semaLock: print(\u0026#34;run sub thread %s\u0026#34; % thName) time.sleep(3) if __name__ == \u0026#34;__main__\u0026#34;: semaLock = threading.Semaphore(2) for i in range(maxSubThreadNumber): subThreadIns = threading.Thread(target=task) subThreadIns.start() Lock Relationships  The above 5 types of locks can be said to be based on synchronous locks to do, which you can find from the source code.\nFirst, let\u0026rsquo;s look at the RLock recursive lock. The implementation of recursive lock is very simple, it maintains an internal counter, when the counter is not 0, the thread cannot be switched by I/O operations and time polling mechanism. This is not the case when the counter is 0:\ndef __init__(self): self._block = _allocate_lock() self._owner = None self._count = 0 # counter The Condition conditional lock actually has two locks inside, a bottom-level lock (synchronous lock) and a high-level lock (recursive lock).\nThere are two ways to unlock the low-level lock. Using the wait() method temporarily unlocks the bottom-level lock and adds a high-level lock, and only when it receives a notfiy() from another thread does it unlock the high-level lock and re-lock the low-level lock, which means that the condition lock is implemented based on the constant switching between synchronous and recursive locks.\ndef __init__(self, lock=None): if lock is None: lock = RLock() # You can see that conditional locking is internally based on recursive locking, which in turn is based on synchronous locking self._lock = lock self.acquire = lock.acquire self.release = lock.release try: self._release_save = lock._release_save except AttributeError: pass try: self._acquire_restore = lock._acquire_restore except AttributeError: pass try: self._is_owned = lock._is_owned except AttributeError: pass self._waiters = _deque() Event event locks are internally based on conditional locks to do the following.\nclass Event: def __init__(self): self._cond = Condition(Lock()) # Instantiates a conditional lock. self._flag = False def _reset_internal_locks(self): # private! called by Thread._reset_internal_locks by _after_fork() self._cond.__init__(Lock()) def is_set(self): \u0026#34;\u0026#34;\u0026#34;Return true if and only if the internal flag is true.\u0026#34;\u0026#34;\u0026#34; return self._flag isSet = is_set Semaphore semaphore locks are also internally based on conditional locks to do the following.\nclass Semaphore: def __init__(self, value=1): if value \u0026lt; 0: raise ValueError(\u0026#34;semaphore initial value must be \u0026gt;= 0\u0026#34;) self._cond = Condition(Lock()) # As you can see, a conditional lock is instantiated here self._value = value Basic Exercises  Application of conditional locks Requirement: An empty list with two threads taking turns adding values to it (one adding an even number, one adding an odd number), so that the values in the list are 1 - 100, and are ordered.\nimport threading lst = [] def even(): \u0026#34;\u0026#34;\u0026#34;Add even numbers\u0026#34;\u0026#34;\u0026#34;\u0026#34; with condLock: for i in range(2, 101, 2): # Determine if the current list is exhausted at length 2 # If so, add an odd number # If not, add an even number if len(lst) % 2 ! = 0: # Add an even number lst.append(i) # Add the value first condLock.notify() # tell the other thread that you can add the odd number, but here you don\u0026#39;t immediately hand over execution condLock.wait() # hand over execution rights and wait for another thread to notify to add an even number else: # Add an odd number condLock.wait() # surrender execution rights and wait for another thread to notify to add an even number lst.append(i) condLock.notify() condLock.notify() def odd(): \u0026#34;\u0026#34;\u0026#34;add odd numbers\u0026#34;\u0026#34;\u0026#34;\u0026#34; with condLock: for i in range(1, 101, 2): if len(lst) % 2 == 0: lst.append(i) condLock.notify() condLock.wait() condLock.notify() if __name__ == \u0026#34;__main__\u0026#34;: condLock = threading.Condition() addEvenTask = threading.Thread(target=even) addOddTask = threading. addEvenTask.start() addOddTask.start() addEvenTask.join() addOddTask.join() print(lst) Application of event locks There are 2 task threads to play Li Bai and Du Fu, how can we make them reply to each other in one sentence? The text is as follows.\nDu Fu: Old Li, come drink! Li Bai: Old Du, I can't drink anymore! Du Fu: Old Li, one more pot? Du Fu: ... old Li? Li Bai: Hoo hoo hoo... fell asleep... The code is as follows.\nimport threading def libai(): event.wait() print(\u0026#34;Li Bai: Lao Du ah, do not drink I can not drink!\u0026#34;) event.set() event.clear() event.wait() print(\u0026#34;Li Bai: Hoo hoo hoo... Sleeping...\u0026#34;) def dufu(): print(\u0026#34;Dufu: Old Li, come drink!\u0026#34;) event.set() event.clear() event.wait() print(\u0026#34;Du Fu: Old Li ah, another pot?\u0026#34;) print(\u0026#34;Du Fu: ... Old Li?\u0026#34;) event.set() if __name__ == \u0026#39;__main__\u0026#39;: event = threading.Event() t1 = threading.Thread(target=libai) Thread(target=dufu) t1.start() t2.start() t1.join() t2.join() Original link: https://mp.weixin.qq.com/s/4PBaW4mT7tFhqcgurAM7-A\n","date":"20 Nov, 2021","image":"images/blog/python.png","permalink":"https://codelink.ai/blog/python/explaining-the-5-python-thread-locks/","tags":["multi-threading"],"title":"Explaining the 5 Python thread locks"},{"categories":["python"],"contents":" I recently got a new task: I need to split a video of an event into smaller segments of two minutes or less for posting on a short video platform.\nI thought it would be a one-time job, but it turned out to be too big and too short to handle manually, so Python came to my rescue once again.\nSo what are you waiting for, let\u0026rsquo;s do it!\nThe most important thing  No matter what you do, you have to analyze what\u0026rsquo;s the most important thing, then focus on attacking it, and then move on to the most important thing.\nFor our task, it is not a big project, but then, we still need to find the most important thing to start, step by step, and eventually the whole problem will be solved.\nAs a whole, we need to read video files from a directory, then, crop each video file, and finally save the processed files.\nWhat is the most important thing in this process? I think, it\u0026rsquo;s video cropping. If you can\u0026rsquo;t crop the video easily, all the other work is in vain, right?\nCrop video  Nowadays, short videos are very popular, and there are many video editing software with rich features, and all we need is the cropping function, and we need to call it programmatically, so there is nothing better than ffmpeg.\nffmpeg is a command line tool that is powerful and can be called programmatically.\nDownload the version for your operating system from the ffmpeg website.\nAfter downloading, unzip it into a directory and configure the bin in the directory to the environment variables. Then open a command line and type.\n\u0026gt; ffmpeg -version ffmpeg version 2021-10-07-git-b6aeee2d8b-full_build- ... Test it out and it shows the version information, which means it\u0026rsquo;s configured.\nNow read the documentation and find that the command to split the video file is\nffmpeg -i [filename] -ss [starttime] -t [length] -c copy [newfilename]  i is the file to be cropped ss is the start time of the crop t is the end time or length of the crop c is the storage of the cropped file  Okay, write a function in Python:\nimport subprocess as sp def cut_video(filename, outfile, start, length=90): cmd = \u0026#34;ffmpeg -i %s-ss %d-t %d-c copy %s\u0026#34; % (filename, start, length, outfile) p = sp.Popen(cmd, shell=True) p.wait() return  Defines a function that passes in the information needed by ffmpeg via parameters Write the crop command as a string template and replace the arguments into it Execute the command with subprocess\u0026rsquo;s Popen, where the argument shell=True means execute the command as a whole p.wait() is important, because cropping takes a while and is executed in a separate process, so you need to wait for the execution to finish before doing subsequent work, otherwise you may not find the cropped file  So the video cropping is done, and then we\u0026rsquo;ll see what\u0026rsquo;s most important.\nCalculating segments  When video cropping, you need some parameters, especially the start time, how to determine it? If this thing is not done properly, the cropping work will be very troublesome.\nSo take a look at how to calculate the crop segments.\nI need to crop the video into small segments of one and a half minutes, then will need to know the duration of the target video file.\nGet the video length  How to get the length? ffmpeg provides another command \u0026ndash; ffprobe.\nAfter looking around, one can synthesize a command to get.\n\u0026gt; ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 -i a.flv 920.667 The command is rather complicated, you can ignore the other parameters first, just pass in the video file to be analyzed. The result of the command is to display the length of a line of video files.\nSo you can write a function.\nimport subprocess as sp def get_video_duration(filename): cmd = \u0026#34;ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 -i %s\u0026#34; % filename p = sp.Popen(cmd, stdout=sp.PIPE, stderr=sp.PIPE) p.wait() strout, strerr = p.communicate() # remove the last carriage return ret = strout.decode(\u0026#34;utf-8\u0026#34;).split(\u0026#34;\\n\u0026#34;)[0] return ret  The function has only one parameter, the video file path Synthesize the command statement and replace the video file path with it use subprocess to execute, note that here you need to set the output of the command execution use wait to wait for the command execution to finish Extract the output result by communicate Extracts the length of the video file from the result and returns  Segmentation  After getting the length of the video, determine the length of each segment, and then calculate how many segments are needed.\nThe code is simple.\nimport math duration = math.floor(float(get_video_duration(filename))) part = math.ceil(duration / length) Note that when calculating the segment, you need to do an upward rounding, i.e., use ceil, to include the last bit of the tail.\nOnce you have the number of segments you need, use a loop to calculate the start time of each segment.\nGetting the files Because there are many files to process, it is necessary to automatically fetch the files that need to be processed.\nThe method is simple and commonly used. You can generally use os.walk to get the files recursively, or you can write your own, depending on the actual situation.\nfor fname in os.listdir(dir): fname = os.path.join(dir, os.path.join(dir, fname)) basenames = os.path.basename(fname).split(\u0026#39;.\u0026#39;) mainname = basenames[0].split(\u0026#34;_\u0026#34;)[0] ... Provide the directory where the video file is located, get the files in the directory via os.listdir, and then, synthesize the absolute path of the file, because it is easier to call the crop command when you need the absolute path.\nGet the file name, in order to name the cropped file later.\nCode integration  Now that each part is written, the code can be integrated as follows\ndef main(dir): outdir = os.path.join(dir, \u0026#34;output\u0026#34;) if not os.path.exists(outdir): os.mkdir(outdir) for fname in os.listdir(dir): fname = os.path.join(dir, os.path.join(dir, fname)) if os.path.isfile(fname): split_video(fname, outdir)  The main method is a post-integration method First create a culled storage directory and put it in the output directory of the video file directory After getting the files by listdir, each file is processed, which determines whether it is a file or not Call the split_video method to start cropping a video file  Summary  Overall, this is a very simple application, the core function is to call a ffmpeg command.\nWhat is more important than the technology is how to analyze and break down a project and where to start.\nThe way to start here is to keep looking for the most important things, and to use the most important things as clues to keep moving forward and eventually solve the whole problem in a bottom-up way.\nI hope this article has inspired you.\nSource:\n ffmpeg: http://ffmpeg.org/ https://mp.weixin.qq.com/s/Ts3isK0qLU5xHfYhVOtBPg  ","date":"28 Oct, 2021","image":"images/blog/ffmpeg.png","permalink":"https://codelink.ai/blog/python/the-magic-tool-ffmpeg-operating-video-with-extreme-comfort/","tags":["learning","ffmpeg"],"title":"The magic tool ffmpeg -- editing video with extreme comfort"},{"categories":["python"],"contents":" Today we share with you 3 relatively cold knowledge.\nThe first one: the magic dictionary key  some_dict = {} some_dict[5.5] = \u0026#34;Ruby\u0026#34; some_dict[5.0] = \u0026#34;JavaScript\u0026#34; some_dict[5] = \u0026#34;Python\u0026#34; Output:\n\u0026gt;\u0026gt;\u0026gt; some_dict[5.5] \u0026#34;Ruby\u0026#34; \u0026gt;\u0026gt;\u0026gt; some_dict[5.0] \u0026#34;Python\u0026#34; \u0026gt;\u0026gt;\u0026gt; some_dict[5] \u0026#34;Python\u0026#34; \u0026ldquo;Python\u0026rdquo; eliminates the existence of \u0026ldquo;JavaScript\u0026rdquo;?\n💡 Description:\n  The Python dictionary determines whether two keys are identical by checking for key equality and comparing hash values.\n  Immutable objects with the same value always have the same hash value in Python.\n  Note: Objects with different values may also have the same hash (hash collision).\n\u0026gt;\u0026gt; 5 == 5.0 True \u0026gt;\u0026gt;\u0026gt; hash(5) == hash(5.0) True When executing the statement some_dict[5] = \u0026quot;Python\u0026quot;, the existing value \u0026ldquo;JavaScript\u0026rdquo; is overwritten by \u0026ldquo;Python\u0026rdquo; because Python recognizes 5 and 5.0 as the same key of some_dict.\nSecond: return in exception handling  def some_func(): try: return \u0026#39;from_try\u0026#39; finally: return \u0026#39;from_finally\u0026#39; Output:\n\u0026gt;\u0026gt;\u0026gt; some_func() \u0026#39;from_finally\u0026#39; 💡 Description:\n  When return, break or continue is executed in the try of the \u0026ldquo;try\u0026hellip;finally\u0026rdquo; statement, the finally clause is still executed.\n  The return value of the function is determined by the last executed return statement. Since the finally clause will always be executed, the return in the finally clause will always be the last statement executed.\n  Third: Determination of identical objects  class WTF: pass Output:\n\u0026gt;\u0026gt;\u0026gt; WTF() == WTF() # Two different objects should not be equal False \u0026gt;\u0026gt;\u0026gt; WTF() is WTF() # also not the same False \u0026gt;\u0026gt;\u0026gt; hash(WTF()) == hash(WTF()) # The hash values should also be different True \u0026gt;\u0026gt;\u0026gt; id(WTF()) == id(WTF()) True 💡 Description:\n  When the id function is called, Python creates an object of class WTF and passes it to the id function. The id function then gets its id value (that is, its memory address), and discards the object. The object is then destroyed.\n  When we do this twice in a row, Python allocates the same memory address to the second object. Because the id function (in CPython) uses the object\u0026rsquo;s memory address as the object\u0026rsquo;s id value, the id values of both objects are the same.\n  In summary, an object\u0026rsquo;s id value is unique only for the life of the object. After the object is destroyed, or before it is created, other objects can have the same id value.\n  So why does the is operation result in False? Let\u0026rsquo;s look at this code.\n  class WTF(object): def __init__(self): print(\u0026#34;I\u0026#34;) def __del__(self): print(\u0026#34;D\u0026#34;) Output:\n\u0026gt;\u0026gt;\u0026gt; WTF() is WTF() I I D D False \u0026gt;\u0026gt;\u0026gt; id(WTF()) == id(WTF()) I D I D True As you can see, the order of object destruction is the reason for all the differences.\nOriginal link: https://github.com/leisurelicht/wtfpython-cn\n","date":"19 Oct, 2021","image":"images/blog/python.png","permalink":"https://codelink.ai/blog/python/there-are-3-incredible-return-functions-in-python/","tags":["learning"],"title":"There are 3 incredible return functions in Python"},{"categories":["web development"],"contents":" There is no doubt: making good use of online resources and tools can speed up development, improve quality, and make life more Chill 😎~.\nThis article brings you 10 great free web resources for front-end developers ⭐ 😎 (๑-̀ㅂ-́)و✧\n1. Undraw  If you need free SVG illustrations for your website, don\u0026rsquo;t miss Undraw!\nSVG illustration resources are huge, with search function available; and, you can also customize the color scheme of the illustration, simply too NICE ~\nA large number of resources, support search 🔍 Feel free to change the color scheme 🌈\n2. Error 404  I don\u0026rsquo;t know where you would normally go to find 404 page material ~\nNow you have one more option: Error 404\nCool, cool, cool!\n3. Squoosh  Compressed images!\nCompared to tinypng has better compression effect.\ntinypng compression\nSquoosh compression\nCompression effect: the former is 80%, the latter is 95%; the final result is also good ~👍\nWhy not try ?\n4. DevDocs  DevDocs, as the name suggests, is the technical documentation for web development and is a very good learning manual!\nOther than that, I like the UI! Also supports adding common technical documents, changing the theme, etc. ~\n5. iHateRegex  If you hate regular expressions, then do not miss this site (ˉ▽ˉ;)\u0026hellip;\nNot only that, but there are also detailed illustrations! Damn, it\u0026rsquo;s so well done ╮(╯▽▽)╭\n6. Carbon  People often ask: \u0026ldquo;How do I generate such nice code snippets?\u0026rdquo; and the answer is in Carbon!\nYou can generate code snippets for various themes and languages and export them as images or copy them to other platforms, it\u0026rsquo;s really nice to use 👌 👌 👌 comfortable~~\n7. Dribbble  For web design inspiration, look no further than Dribbble!!!\nWhen you see other people\u0026rsquo;s backend designs, you want to go back and rip up your own 🐶\n8. Animista  Css animation, copy the code and you can use it! No installation, doesn\u0026rsquo;t it sound good?\n9. Shape Divider  You can generate all kinds of dividers and export them in SVG format.\nFancy, I like it (❤ ω ❤)\n10. Notion  If you need a platform for note-taking, we recommend one option: Notion\nQuick Notes, TaskList, Diary, Reading List, all types, everything, recommended~\nSource: https://mp.weixin.qq.com/s/n71kwdVSLoJatAmCs-djJQ\n","date":"17 Oct, 2021","image":"images/blog/post-3.webp","permalink":"https://codelink.ai/blog/web-development/recommend-10-very-wow-web-resources-to-the-front-end-developers/","tags":["tools","frontend","web"],"title":"Recommend 10 very 'wow' Web 'resources' to the front-end developers"},{"categories":["devops"],"contents":" ag faster than grep, ack recursive search file content.\n tig Interactive view of git projects in character mode, can replace git command.\n mycli mysql client, support syntax highlighting and command completion, similar to ipython, can replace mysql command.\n jq json file processing and formatting display, support highlighting, can replace python -m json.tool.\n shellcheck shell script static checking tool, can identify syntax errors and irregular writing style.\n fzf command line fuzzy search tool, can interactively and intelligently search and select files or content, with terminal ctrl-r history command search is perfect.\n PathPicker(fpp) Automatically identifies directories and files in the command line output, supports interactive, very useful with git.\nRun the following command.\ngit diff HEAD~8 --stat | fpp   htop Provides a more beautiful and convenient process monitoring tool, replacing the top command.\n glances A more powerful alternative to htop / top.\nhtop replaces top, glances replaces htop: htop replaces top, glances replaces htop.\nInformation is much richer and more complete than htop, isn\u0026rsquo;t it? In addition to the command line view, glances also provides a page service that allows you to view the status of a server from the page at any time.\n axel a multi-threaded download tool that can replace curl and wget when downloading files.\naxel -n 20 http://centos.ustc.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso   sz/rz interactive file transfer, very good for transferring files under multiple jumpers, no need to transfer at one level.\n cloc code statistics tool, can count the number of empty lines of code, comment lines, programming language.\n tmux terminal reuse tool, instead of screen, nohup.\n script/scriptreplay Terminal session recording.\n# recording script -t 2\u0026gt;time.txt session.typescript # your commands # end of recording exit # Playback scriptreplay -t time.txt session.typescript  multitail Often you have more than one log file to monitor, what should you do? It takes up too much space to open multiple tabs in the terminal software, try this tool.\nSource: https://mp.weixin.qq.com/s/PkIUB3zTYpTXKcrgZg4f-A\n","date":"13 Sep, 2021","image":"images/blog/linux.png","permalink":"https://codelink.ai/blog/devops/must-have-linux-tools/","tags":["tools"],"title":"These Linux tools are must-haves! Which one have you used?"},{"categories":["web development"],"contents":" Programmers need to deal with lots of diagrams in their daily life: flowcharts, architecture diagrams, interaction diagrams, functional module diagrams, UML class diagrams, deployment diagrams, various visual diagrams, and so on.\nThrough the form of diagrams, one can better display the system, more clearly express their ideas, easy to understand; also can exercise their own drawing skills, really a multi-benefit thing.\nToday I will share my common diagramming software and a little tips.\n Commonly used diagrams I usually draw more flowcharts, interaction diagrams and architecture diagrams, and I usually use Draw.io, a free online web drawing tool, to get it done.\nThe main reasons why I chose this drawing tool are as follows.\n1. ease of use No need to log in, directly access the web page, you can directly use a large number of templates to create new projects.\nSelect a template Then enter the editing page, select the graphics you need on the left, drag them to the drawing area for editing, and then modify the style on the right.\nDraw.io online drawing You can directly search for the needed graphics, such as the server, and of course, you can also directly paste local or network images.\n2. Beautiful style Draw.io provides several default themes and preset graphic styles, such as my favorite hand-drawn style.\nOf course, you can also customize the graphics with a high degree of flexibility, and if you are familiar with interface development, you can even edit the values of the graphics properties directly.\nProperty editing 3. Rich Export You can export the drawing to image, vector, PDF, HTML document, etc. with one click.\nWhat I like most is that it can generate online web pages directly and share them with others for quick browsing, and it also provides a small toolbar for zooming, screenshotting, printing and quick editing.\nOnline Browsing In addition, you can export your drawings as embedded web pages! This allows us to seamlessly integrate all kinds of drawings directly into the web pages we develop.\nExport to embedded format 4. Easy storage and import You can save your drawing as a local file at any time, or store it in an online space like GitHub; when you need to edit it again, you can import it from the same place.\nFor those of you who are used to using GitHub to store and share your code, this is very useful and means you can collaborate on drawings with other students.\nIn addition to Draw.io, I also like EdrawMax, a local mapping software that is also very powerful.\n ER Diagram Backend development students may come across ER diagrams, which are often used in database design to represent the properties and relationships of data.\nI usually don\u0026rsquo;t draw this stuff manually, I just use database management software (such as Navicat, JetBrains DataGrip, etc.) to generate it based on the existing library tables when needed.\nAutomatic generation of ER diagrams\n UML Class Diagram UML class diagrams are used to represent the relationship between classes and help to quickly understand the design structure of the whole system.\nLike ER diagrams, I don\u0026rsquo;t draw this stuff myself, it\u0026rsquo;s exhausting. Generally, I use an IDE (such as JetBrains IDEA) to automatically generate UML class diagrams based on the code, as shown in the figure.\nAutomatic generation of UML class diagrams\nThis is not better than drawing your own?\n Visual diagram Charts can show data and trends more visually, and play a pivotal role in PPT presentations.\nCommon charts include bar charts, column charts, pie charts, line charts, and so on.\nAlthough Excel and PPT can draw charts, they are relatively ugly, so I recommend Flourish, a website that generates visual charts online.\nIt has a rich set of built-in templates for chart types.\nAfter selecting a chart type, you can configure the data to be displayed and the chart style.\nI like the dynamic charts and URL sharing feature that allows you to quickly create real-time charts with dynamically changing values.\nOnce you\u0026rsquo;re done, you can share the web address with a single click for others to view or embed in your own web page.\nExport chart pages\n Mind Map The most common mind mapping software I use is XMind, which is easy to use and rich in topics.\nXMind\nI usually don\u0026rsquo;t write mind maps directly in XMind, but I write a Markdown document first, and then import the document directly into XMind to automatically generate mind maps based on headings, lists, and other elements.\nImporting Markdown\nOtherwise, it would be really tiring to edit them one by one.\n Postscript Drawing diagrams is something that can be imitated, more reading and more drawing, practice makes perfect.\nWhen you need to draw a diagram, if you can\u0026rsquo;t draw, no ideas, just go online and search for similar diagrams drawn by others, and just draw a tiger from a cat.\nSource: https://mp.weixin.qq.com/s/9npqlkFC5ZZyQrikOv9uJQ\n","date":"31 Aug, 2021","image":"images/blog/uml.jpeg","permalink":"https://codelink.ai/blog/web-development/programmers-commonly-used-drawing-software-and-tips/","tags":["tools","drawing"],"title":"Programmers commonly used drawing software and tips"},{"categories":["devops"],"contents":" The ls command is the most commonly used command in Linux. ls is short for list, and by default ls is used to print out a list of the current directory.\nIf ls specifies another directory, then it will display a list of files and folders in the specified directory. The ls command allows you to view not only the files contained in a Linux folder but also the file permissions (including directory, folder, and file permissions), directory information, and more.\nThe ls command is used a lot in daily Linux operations!\n1. Command format.  ls [option] [directory name] 2. Command function.  List all the subdirectories and files in the target directory. 3.\n3. Common parameters:  -a, -all List all the files in the directory, including the implicit files starting with . including the implied files starting with . -A is the same as -a, but does not list \u0026quot;.\u0026quot; (for the current directory) and \u0026quot;...\u0026quot; (for the parent of the current directory). -c with -lt: sort by ctime and show ctime (when the file status was last changed) with -l: show ctime but sort by name otherwise: sort by ctime -C List items from top to bottom in each column -WHEN can be 'never', 'always' or 'auto ' either -d, -directory Displays directories as files, instead of showing the files under them. -D, -dired produces results suitable for use in Emacs' dired mode -f does not sort the output files, the -aU option takes effect and the -lst option fails -g is similar to -l, but does not list the owner -G, -no-group does not list any information about groups -h, -human-readable lists the file size in an easy to understand format (e.g. 1K 234M 2G) -si is similar to -h, but the file size is taken to the power of 1000 instead of 1024 -H, -dereference-command-line Use the real destination indicated by the symbolic link in the command line -indicator-style=style Specify that each item name is followed by the indicator \u0026lt;style\u0026gt;: none (default), classify (-F), file-type (-p) -i, -inode prints out the inode number of each file -I, -ignore=style Do not print out any items that match the shell's universal character \u0026lt;style -k i.e. -block-size=1K, indicates the size of the file in k bytes. -l lists the file permissions, owner, file size, and other information in detail, in addition to the file name. -L, -dereference When displaying information about files with symbolic links, display information about the object indicated by the symbolic link instead of the symbolic link itself -m All items are separated by commas and fill the entire line width -o is similar to -l, showing detailed information about the file except for group information. -r, -reverse in reverse order -R, -recursive lists all subdirectory levels simultaneously -s, -size list all files in block size -S Sort by file size -sort=WORD The following WORDs are available and the corresponding options they represent. extension -X status -c none -U time -t size -S atime -u time -t access -u version -v use -u -t Sort by file modification time -u with -lt: Show access time and sort by access time with -l: show access time but sort by name otherwise: sort by access time -U no sorting; list items in the original order of the file system -v: Sort by version -w, -width=COLS specify your own screen width instead of using the current value -x list items line by line instead of column by column -X Sort by extension -1 List only one file per line -help Display this help message and leave -version Displays version information and leaves 4. Common examples Example 1: List the details of all files and directories under the /home/codelinkai folder Command: ls -l -R /home/codelinkai\nWhen you use ls command, you should pay attention to the format of the command: after the command prompt, the keyword of the command is first, followed by the command parameters, and there is a short horizontal line \u0026ldquo;-\u0026rdquo; before the command parameters, all the command parameters have specific functions, and you can choose one or more parameters according to your needs.\nIn the above command ls -l -R /home/codelinkai, ls is the command keyword, -l -R is the parameter, and /home/codelinkai is the parameter. \u0026ldquo;/home/codelinkai\u0026rdquo; is the object of the command. In this command, two parameters are used, \u0026ldquo;l\u0026rdquo; and \u0026ldquo;R\u0026rdquo;, but you can also use them together as follows.\nCommand: ls -lR /home/codelinkai\nThe result of this form is exactly the same as the above command. In addition, if the operation object of the command is located in the current directory, you can operate on the operation object directly; if it is not in the current directory then you need to give the full path of the operation object, for example, in the above example, my current folder is the codelinkai folder, and I want to operate on the codelinkai file under the home folder, I can directly type ls -lR codelinkai, or I can use ls -lR /home/codelinkai.\nExample 2: To list the details of all the directories starting with \u0026ldquo;t\u0026rdquo; in the current directory, you can use the following command. Command: ls -l t*\nYou can view the information of all the files in the current directory whose file name starts with \u0026ldquo;t\u0026rdquo;. In fact, in the command format, the contents inside the square brackets can be omitted. For the command ls, if you omit the command parameters and operation objects and type \u0026ldquo;ls\u0026rdquo; directly, the contents of the current working directory will be listed.\nExample 3: List only the subdirectories under the file Command: ls -F /opt/soft |grep /$\nList the subdirectories under the /opt/soft file\nOutput:\n[root@localhost opt]# ls -F /opt/soft |grep /$ jdk1.6.0_16/ subversion-1.6.1/ tomcat6.0.32/ Command: ls -l /opt/soft | grep \u0026quot;^d\u0026quot;\nList the details of the subdirectories under the /opt/soft file\nOutput:\n[root@localhost opt]# ls -l /opt/soft | grep \u0026#34;^d\u0026#34; drwxr-xr-x 10 root root 4096 09-17 18:17 jdk1.6.0_16 drwxr-xr-x 16 1016 1016 4096 10-11 03:25 subversion-1.6.1 drwxr-xr-x 9 root root 4096 2011-11-01 tomcat6.0.32 Example 4: List all the files in the current working directory whose names start with s. The newer the file, the later it is, you can use the following command. Command: ls -ltr s**\nOutput:\n[root@localhost opt]# ls -ltr s* src: Total 0 script: Total 0 soft: Total 350644 drwxr-xr-x 9 root root 4096 2011-11-01 tomcat6.0.32 -rwxr-xr-x 1 root root 81871260 09-17 18:15 jdk-6u16-linux-x64.bin drwxr-xr-x 10 root root 4096 09-17 18:17 jdk1.6.0_16 -rw-r--r-- 1 root root 205831281 09-17 18:33 apache-tomcat-6.0.32.tar.gz -rw-r--r-- 1 root root 5457684 09-21 00:23 tomcat6.0.32.tar.gz -rw-r--r-- 1 root root 4726179 10-10 11:08 subversion-deps-1.6.1.tar.gz -rw-r--r-- 1 root root 7501026 10-10 11:08 subversion-1.6.1.tar.gz drwxr-xr-x 16 1016 1016 4096 10-11 03:25 subversion-1.6.1 Example 5: List all files and directories in the current working directory; add \u0026ldquo;/\u0026rdquo; after the directory name, and add \u0026ldquo;\u0026quot; after the executable file name Command: ls -AF\nOutput:\n[root@localhost opt]# ls -AF log/ script/ soft/ src/ svndata/ web/ Example 6: Calculate the number of files and directories in the current directory Command:\nls -l * |grep \u0026#34;^-\u0026#34;|wc -l -number of files ls -l * |grep \u0026#34;^d\u0026#34;|wc -l -number of directories Example 7: List the absolute path of a file in ls Command: ls | sed \u0026quot;s:^:pwd/:\u0026quot;\nOutput:\n[root@localhost opt]# ls | sed \u0026#34;s:^:`pwd`/:\u0026#34;  /opt/log /opt/script /opt/soft /opt/src /opt/svndata /opt/web Example 8: List the absolute paths of all files (including hidden files) in the current directory, without recursion for directories Command: find $PWD -maxdepth 1 | xargs ls -ld\nOutput:\n[root@localhost opt]# find $PWD -maxdepth 1 | xargs `ls` -ld drwxr-xr-x 8 root root 4096 10-11 03:43 /opt drwxr-xr-x 2 root root 4096 2012-03-08 /opt/log drwxr-xr-x 2 root root 4096 2012-03-08 /opt/script drwxr-xr-x 5 root root 4096 10-11 03:21 /opt/soft drwxr-xr-x 2 root root 4096 2012-03-08 /opt/src drwxr-xr-x 4 root root 4096 10-11 05:22 /opt/svndata drwxr-xr-x 4 root root 4096 10-09 00:45 /opt/web Example 9: Recursively list the absolute paths of all files (including hidden files) in the current directory Command: find $PWD | xargs ls -ld\nExample 10: Specify the file time output format** Command: ls -tl -time-style=full-iso\nOutput:\n[root@localhost soft]# `ls` -tl --time-style=full-iso  Total 350644 drwxr-xr-x 16 1016 1016 4096 2012-10-11 03:25:58.000000000 +0800 subversion-1.6.1 Command: ls -ctl -time-style=long-iso\nOutput:\n[root@localhost soft]# `ls` -ctl --time-style=long-iso Total 350644 drwxr-xr-x 16 1016 1016 4096 2012-10-11 03:25 subversion-1.6.1 Extensions  Open /etc/bashrc and add the following line:\nalias ls=\u0026#34;ls --color\u0026#34; The next time you start bash, you will be able to display a colored list of directories like in Slackware, where the colors mean the following:\n  blue\u0026ndash;\u0026gt;directories\n  green\u0026ndash;\u0026gt;executable files\n  red\u0026ndash;\u0026gt;compressed files\n  light blue\u0026ndash;\u0026gt;linked files\n  gray\u0026ndash;\u0026gt;other files\n  Source\n","date":"13 Jul, 2021","image":"images/blog/linux.png","permalink":"https://codelink.ai/blog/devops/a-linux-command-a-day-1-ls-command/","tags":["learning","linux"],"title":"A Linux command a day (1): ls command"},{"categories":["devops"],"contents":" 1. Operating system  Microsoft Windows：Assembly -\u0026gt; C -\u0026gt; C++\nNote: Once in the smartphone operating system (Windows Mobile) to consider mixing the program written in C#, such as soft keyboard, the results are too slow to write out the program, it is impossible to merge with other modules, and finally back to C++ rewrite.\nI believe many friends know Windows Vista, the system development early Bill Gates wanted to write all in C#, but finally because of the slow implementation and give up, the results of the previous countless software engineers work day and night results overnight was declared null and void.\nLinux: C\nApple MacOS: mainly C, partly C++.\nNote: The language used before is rather mixed, the earliest is assembly and Pascal.\nSun Solaris: C\nHP-UX: C\nSymbian OS: Assembly, mainly C++ (Nokia phones)\nGoogle Android: launched in 2008: C (there are rumors that the operating system is developed in Java, but just recently launched a native C SDK)\nRIM BlackBerry OS 4.x: BlackBerry C++\n2. GUI layer  Microsoft Windows UI: C++\nApple MacOS UI (Aqua): C++\nGnome (one of the Linux GUIs, Bigfoot): C and C++, but mainly C\nKDE (Linux GUI): C++\n3. Desktop search tools  Google Desktop Search: C++\nMicrosoft Windows Desktop Search: C++\nBeagle (under Linux/Windows/UNIX): C# (based on open source .net: Mono)\n4. Office software  Microsoft Office: in Assembly -\u0026gt; C -\u0026gt; stable in C++\nSun Open Office: part of JAVA (external interface), mainly for C (open source, can download its source code)\nCorel Office/WordPerfect Office: tried Java in 1996, abandoned the following year, back to C/C\nAdobe Systems Acrobat Reader/Distiller: C++\n5. Relational database  Oracle: assembly, C, C, Java. mainly for C\nMySQL: C++\nIBM DB2: Assembly, C, C++, but mainly C\nMicrosoft SQL Server: Assembly -\u0026gt; C-\u0026gt;C++\nIBM Informix: Assembly, C, C++, but mainly C\nSAP DB/MaxDB: C++\n6. Web Browsers/Browsers  Microsoft Internet Explorer: C++\nMozilla Firefox: C++\nNetscape Navigator: The code of Netscape browser was written in C, and Netscape engineers, all bought to Java (see M. Cusumano book and article) It was too slow and abandoned. Mozilla, the next version, was later developed using C++.\nSafari: (released in January 2003) C++\nGoogle Chrome: (2008 release) C++\nSun HotJava: Java (died in 1999)\nOpera: C++ (more popular on cell phones)\nOpera Mini: Opera Mini (2007) has a very funny architecture, and is indeed using both C++ and Java. The browser is split in two parts, an ultra thin (less than The first uses Java and receives the page under the OBML format, the latter reuses The first uses Java and receives the page under the OBML format, the latter reuses classical Opera (C++) rendering engine plus Opera\u0026rsquo;s Small Screen Rendering, on the server. This allows Opera to penetrate various J2ME-enabled portable devices, such as phones, while preserving excellent response time. execution.\nMosaic: The originator (dead) C language\n7. Email client  Microsoft Outlook: C++\nIBM Lotus Notes: Java\nFoxmail: Delphi\n8. Integrated software development environment/IDE  Microsoft Visual Studio: C++\nEclipse: Java (its graphical interface SWT based on C/C++)\nCode::Blocks: C++\nVolcano Chinese: C++\nVolcano Mobile: C++\n9. Virtual Machine  Net CLR (virtual machine for .NET): C++\nJava Virtual Machine (JVM): Java Virtual Machine : C++\n10. ERP software (enterprise applications)  SAP mySAP ERP: C, after the main \u0026ldquo;ABAP/4\u0026rdquo; language\nOracle Peoplesoft: C++ -\u0026gt; Java\nOracle E-Business Suite: Java\n11. Business Intelligence (Business Intelligence)  Business Objects: C++\n12. Graphics Processing  Adobe Photoshop: C++\nThe GIMP: C\n13. Search engine  Google: assembly and C++, but mainly for C++\n14. Famous website  eBay: C++ in 2002, after the main move to Java\nfacebook: C++ and PHP\nThis line is only about facebook, not its plugins. Plugins can be developed in many different technologies, thanks to facebook\u0026rsquo;s ORB/application server Thrift contains a compiler coded in C++. facebook people write about Thrift: \u0026ldquo;The multi-language code generation is well suited for search because it allows for application development in an The multi-language code generation is well suited for search because it allows for application development in an efficient server side language (C++) and allows the Facebook PHP-based web application to make calls to the search service using Thrift PHP libraries.\u0026rdquo; Aside the use of C++, facebook has adopted a LAMP architecture.\nAlibaba and Taobao: php-\u0026gt;C++/Java (mainly used)\n15. Games  Assembly, C, C++ Starcraft, Warcraft, CS, Age of Empires, Karting, Legend, World of Warcraft \u0026hellip;. Too many to count, count them yourself!\nThe C++ language is close to the bottom of the system and has the fastest execution speed. For example, your two friends and you respectively play with VB, Java, and C++ written \u0026ldquo;run kart\u0026rdquo;, you play the game written in C++ has been running and playing the end, found that your two friends have not started to run it, that is quite a card ah.\n16. Compiler  Microsoft Visual C++ compiler: C++\nMicrosoft Visual Basic interpretation, compiler: C++\nMicrosoft Visual C#: compiler: C++\ngcc (GNU C compiler): C\njavac (Sun Java compiler): Java\nPerl: C++\nPHP: C\n17. 3D engine  Microsoft DirectX: C++\nOpenGL： C\nOGRE 3D： C++\n18. Web Servers (web services)  Apache: C and C++, but mainly C\nMicrosoft IIS: C++\nTomcat: Java\nJboss: Java\n19. Mail service  **Microsoft Exchange Server**: C-C++ Postfix: C\nhMailServer: C++\nApache James: Java\n20. CD/DVD burning  Nero Burning ROM: C++\nK3B: C++\n21. Media Player  Nullsoft Winamp: C++\nMicrosoft Windows Media Player： C++\n22. Peer to Peer (P2P software)  eMule: C++\nμtorrent: C++\nAzureus: Java (GUI using C/C++ based SWT, Eclipse-like)\n23. Global Positioning System (GPS)  TomTom: C++\nHertz NeverLost: C++\nGarmin: C++\nMotorola VIAMOTO: June 2007, out of service, Java\n24. 3D engine  Microsoft DirectX: C++ (I believe the game students know this, now the highest version is DX11)\nOpenGL: C\nOGRE 3D: C++\n25. Server software  Apache: C\nNginx: C\nIIS: C\n26. Other  OpenStack: Python\nSource: https://mp.weixin.qq.com/s/-znPkfMc8f-2hvqCO0-1jQ\n","date":"04 Jun, 2021","image":"images/blog/coding-language.jpeg","permalink":"https://codelink.ai/blog/devops/what-programming-languages-are-some-famous-softwares-written-in/","tags":["knowledge"],"title":"What programming languages are some famous softwares written in?"}]